{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0ahbRtu2ckUNKaHZ25Ykj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishnu0920/Recommender_System_Models/blob/main/AttentionalFMOnStudentDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH0hRuHHQfx6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('grade_data.csv')\n",
        "\n",
        "# Encode student_id and course_id using LabelEncoder\n",
        "le_student = LabelEncoder()\n",
        "le_course = LabelEncoder()\n",
        "\n",
        "df['student_id'] = le_student.fit_transform(df['student_id'])\n",
        "df['course_id'] = le_course.fit_transform(df['course_id'])\n",
        "\n",
        "# Map course grades to the range [0, 1] for regression\n",
        "df['course_grade'] = df['course_grade'] / 10.0\n",
        "\n",
        "# Train-test split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "1z_K_3DRQrW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to PyTorch tensors\n",
        "train_user = torch.LongTensor(train_df['student_id'].values)\n",
        "train_course = torch.LongTensor(train_df['course_id'].values)\n",
        "train_grade = torch.FloatTensor(train_df['course_grade'].values)\n",
        "\n",
        "test_user = torch.LongTensor(test_df['student_id'].values)\n",
        "test_course = torch.LongTensor(test_df['course_id'].values)\n",
        "test_grade = torch.FloatTensor(test_df['course_grade'].values)\n"
      ],
      "metadata": {
        "id": "wytmW-I2Qu9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionalFactorizationMachine(nn.Module):\n",
        "    def __init__(self, num_users, num_courses, embedding_dim):\n",
        "        super(AttentionalFactorizationMachine, self).__init__()\n",
        "\n",
        "        # Adjust the dimensions of embeddings and linear layer\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.course_embedding = nn.Embedding(num_courses, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim * 2, 1)\n",
        "        self.attention = nn.Linear(embedding_dim * 2, 1)\n",
        "\n",
        "    def forward(self, user, course):\n",
        "        user_emb = self.user_embedding(user)\n",
        "        course_emb = self.course_embedding(course)\n",
        "\n",
        "        # Interaction term\n",
        "        interaction = torch.cat([user_emb, course_emb], dim=1)\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = torch.sigmoid(self.attention(interaction))\n",
        "        attention = attention_weights * interaction\n",
        "\n",
        "        # Concatenate interaction with attention\n",
        "        interaction_attention = interaction + attention\n",
        "\n",
        "        output = self.linear(interaction_attention)\n",
        "        return output.squeeze()\n"
      ],
      "metadata": {
        "id": "86C20V9oQw8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_users = len(le_student.classes_)\n",
        "num_courses = len(le_course.classes_)\n",
        "embedding_dim = 10\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_dataset = TensorDataset(train_user, train_course, train_grade)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Instantiate the model, define loss function and optimizer\n",
        "model = AttentionalFactorizationMachine(num_users, num_courses, embedding_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "LUV6U-0wQzJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the print interval\n",
        "print_interval = 10  # Adjust the interval as needed\n",
        "\n",
        "# Training loop with performance monitoring\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_index, batch in enumerate(train_dataloader):\n",
        "        user, course, grade = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(user, course).squeeze()\n",
        "        loss = criterion(output, grade)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_index % print_interval == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
        "\n",
        "    # Print average loss at the end of each epoch\n",
        "    avg_loss = total_loss / len(train_dataloader.dataset)\n",
        "    print(f'Epoch {epoch}, Average Loss: {avg_loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CvnEQ-4Q1x2",
        "outputId": "8c0a1472-8b7b-4561-945e-e3b185af7e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 0, Loss: 1.2062864303588867\n",
            "Epoch 0, Batch 10, Loss: 1.1519439220428467\n",
            "Epoch 0, Batch 20, Loss: 1.2703150510787964\n",
            "Epoch 0, Batch 30, Loss: 1.0147649049758911\n",
            "Epoch 0, Batch 40, Loss: 1.2423255443572998\n",
            "Epoch 0, Batch 50, Loss: 0.7098916172981262\n",
            "Epoch 0, Batch 60, Loss: 0.6395025849342346\n",
            "Epoch 0, Batch 70, Loss: 0.7679346203804016\n",
            "Epoch 0, Batch 80, Loss: 0.7215143442153931\n",
            "Epoch 0, Batch 90, Loss: 0.6791014075279236\n",
            "Epoch 0, Batch 100, Loss: 0.6706181764602661\n",
            "Epoch 0, Batch 110, Loss: 0.5120199918746948\n",
            "Epoch 0, Batch 120, Loss: 0.49761661887168884\n",
            "Epoch 0, Batch 130, Loss: 0.5822432041168213\n",
            "Epoch 0, Batch 140, Loss: 0.4195896089076996\n",
            "Epoch 0, Batch 150, Loss: 0.4313325881958008\n",
            "Epoch 0, Batch 160, Loss: 0.5568811297416687\n",
            "Epoch 0, Batch 170, Loss: 0.47009938955307007\n",
            "Epoch 0, Batch 180, Loss: 0.38351234793663025\n",
            "Epoch 0, Batch 190, Loss: 0.3767416179180145\n",
            "Epoch 0, Batch 200, Loss: 0.34226834774017334\n",
            "Epoch 0, Batch 210, Loss: 0.3706035017967224\n",
            "Epoch 0, Batch 220, Loss: 0.40858978033065796\n",
            "Epoch 0, Batch 230, Loss: 0.39214953780174255\n",
            "Epoch 0, Batch 240, Loss: 0.2970288097858429\n",
            "Epoch 0, Batch 250, Loss: 0.26903173327445984\n",
            "Epoch 0, Batch 260, Loss: 0.31788724660873413\n",
            "Epoch 0, Batch 270, Loss: 0.22450946271419525\n",
            "Epoch 0, Batch 280, Loss: 0.3237135708332062\n",
            "Epoch 0, Batch 290, Loss: 0.29555457830429077\n",
            "Epoch 0, Batch 300, Loss: 0.2840253710746765\n",
            "Epoch 0, Batch 310, Loss: 0.22510936856269836\n",
            "Epoch 0, Batch 320, Loss: 0.2239580750465393\n",
            "Epoch 0, Batch 330, Loss: 0.21826432645320892\n",
            "Epoch 0, Batch 340, Loss: 0.27267953753471375\n",
            "Epoch 0, Batch 350, Loss: 0.2161691039800644\n",
            "Epoch 0, Batch 360, Loss: 0.17058369517326355\n",
            "Epoch 0, Batch 370, Loss: 0.21828772127628326\n",
            "Epoch 0, Batch 380, Loss: 0.19204182922840118\n",
            "Epoch 0, Batch 390, Loss: 0.22406664490699768\n",
            "Epoch 0, Batch 400, Loss: 0.15118485689163208\n",
            "Epoch 0, Batch 410, Loss: 0.16138458251953125\n",
            "Epoch 0, Batch 420, Loss: 0.20955538749694824\n",
            "Epoch 0, Batch 430, Loss: 0.20928747951984406\n",
            "Epoch 0, Batch 440, Loss: 0.13979081809520721\n",
            "Epoch 0, Batch 450, Loss: 0.18693675100803375\n",
            "Epoch 0, Batch 460, Loss: 0.16789835691452026\n",
            "Epoch 0, Batch 470, Loss: 0.12585823237895966\n",
            "Epoch 0, Batch 480, Loss: 0.14623048901557922\n",
            "Epoch 0, Batch 490, Loss: 0.14322540163993835\n",
            "Epoch 0, Batch 500, Loss: 0.1454475075006485\n",
            "Epoch 0, Batch 510, Loss: 0.16627109050750732\n",
            "Epoch 0, Batch 520, Loss: 0.1513417661190033\n",
            "Epoch 0, Batch 530, Loss: 0.14006243646144867\n",
            "Epoch 0, Batch 540, Loss: 0.14194300770759583\n",
            "Epoch 0, Batch 550, Loss: 0.13766950368881226\n",
            "Epoch 0, Batch 560, Loss: 0.07721110433340073\n",
            "Epoch 0, Batch 570, Loss: 0.12203966826200485\n",
            "Epoch 0, Batch 580, Loss: 0.12415207922458649\n",
            "Epoch 0, Batch 590, Loss: 0.11879874765872955\n",
            "Epoch 0, Batch 600, Loss: 0.1406114399433136\n",
            "Epoch 0, Batch 610, Loss: 0.09753836691379547\n",
            "Epoch 0, Batch 620, Loss: 0.12665332853794098\n",
            "Epoch 0, Batch 630, Loss: 0.0768880844116211\n",
            "Epoch 0, Batch 640, Loss: 0.0830063745379448\n",
            "Epoch 0, Batch 650, Loss: 0.11154584586620331\n",
            "Epoch 0, Batch 660, Loss: 0.08318274468183517\n",
            "Epoch 0, Batch 670, Loss: 0.12213413417339325\n",
            "Epoch 0, Batch 680, Loss: 0.1176825761795044\n",
            "Epoch 0, Batch 690, Loss: 0.08030479401350021\n",
            "Epoch 0, Batch 700, Loss: 0.10269515216350555\n",
            "Epoch 0, Batch 710, Loss: 0.0707743689417839\n",
            "Epoch 0, Average Loss: 0.005137411398313673\n",
            "Epoch 1, Batch 0, Loss: 0.09116865694522858\n",
            "Epoch 1, Batch 10, Loss: 0.08007531613111496\n",
            "Epoch 1, Batch 20, Loss: 0.09225540608167648\n",
            "Epoch 1, Batch 30, Loss: 0.05791806802153587\n",
            "Epoch 1, Batch 40, Loss: 0.09300138056278229\n",
            "Epoch 1, Batch 50, Loss: 0.05927532911300659\n",
            "Epoch 1, Batch 60, Loss: 0.07697874307632446\n",
            "Epoch 1, Batch 70, Loss: 0.07535918056964874\n",
            "Epoch 1, Batch 80, Loss: 0.060973331332206726\n",
            "Epoch 1, Batch 90, Loss: 0.05830112099647522\n",
            "Epoch 1, Batch 100, Loss: 0.0688157007098198\n",
            "Epoch 1, Batch 110, Loss: 0.04158526659011841\n",
            "Epoch 1, Batch 120, Loss: 0.06185750290751457\n",
            "Epoch 1, Batch 130, Loss: 0.07210676372051239\n",
            "Epoch 1, Batch 140, Loss: 0.049526289105415344\n",
            "Epoch 1, Batch 150, Loss: 0.040189240127801895\n",
            "Epoch 1, Batch 160, Loss: 0.06650294363498688\n",
            "Epoch 1, Batch 170, Loss: 0.05443655699491501\n",
            "Epoch 1, Batch 180, Loss: 0.06178775802254677\n",
            "Epoch 1, Batch 190, Loss: 0.07386112213134766\n",
            "Epoch 1, Batch 200, Loss: 0.07993250340223312\n",
            "Epoch 1, Batch 210, Loss: 0.0458308570086956\n",
            "Epoch 1, Batch 220, Loss: 0.04536270350217819\n",
            "Epoch 1, Batch 230, Loss: 0.047181930392980576\n",
            "Epoch 1, Batch 240, Loss: 0.056866418570280075\n",
            "Epoch 1, Batch 250, Loss: 0.06104639172554016\n",
            "Epoch 1, Batch 260, Loss: 0.06479516625404358\n",
            "Epoch 1, Batch 270, Loss: 0.04712383821606636\n",
            "Epoch 1, Batch 280, Loss: 0.05182143300771713\n",
            "Epoch 1, Batch 290, Loss: 0.03637309372425079\n",
            "Epoch 1, Batch 300, Loss: 0.04619145020842552\n",
            "Epoch 1, Batch 310, Loss: 0.056040287017822266\n",
            "Epoch 1, Batch 320, Loss: 0.05322027578949928\n",
            "Epoch 1, Batch 330, Loss: 0.04091924801468849\n",
            "Epoch 1, Batch 340, Loss: 0.03894229605793953\n",
            "Epoch 1, Batch 350, Loss: 0.04286110773682594\n",
            "Epoch 1, Batch 360, Loss: 0.046396203339099884\n",
            "Epoch 1, Batch 370, Loss: 0.026433762162923813\n",
            "Epoch 1, Batch 380, Loss: 0.04642057418823242\n",
            "Epoch 1, Batch 390, Loss: 0.046046119183301926\n",
            "Epoch 1, Batch 400, Loss: 0.053467459976673126\n",
            "Epoch 1, Batch 410, Loss: 0.056756217032670975\n",
            "Epoch 1, Batch 420, Loss: 0.03622892126441002\n",
            "Epoch 1, Batch 430, Loss: 0.037515874952077866\n",
            "Epoch 1, Batch 440, Loss: 0.027670467272400856\n",
            "Epoch 1, Batch 450, Loss: 0.03198038786649704\n",
            "Epoch 1, Batch 460, Loss: 0.03260371834039688\n",
            "Epoch 1, Batch 470, Loss: 0.03638695180416107\n",
            "Epoch 1, Batch 480, Loss: 0.03413799777626991\n",
            "Epoch 1, Batch 490, Loss: 0.02656552568078041\n",
            "Epoch 1, Batch 500, Loss: 0.04049697518348694\n",
            "Epoch 1, Batch 510, Loss: 0.036744434386491776\n",
            "Epoch 1, Batch 520, Loss: 0.029261410236358643\n",
            "Epoch 1, Batch 530, Loss: 0.033784572035074234\n",
            "Epoch 1, Batch 540, Loss: 0.03163284808397293\n",
            "Epoch 1, Batch 550, Loss: 0.02964596077799797\n",
            "Epoch 1, Batch 560, Loss: 0.044087715446949005\n",
            "Epoch 1, Batch 570, Loss: 0.02699434384703636\n",
            "Epoch 1, Batch 580, Loss: 0.02337699756026268\n",
            "Epoch 1, Batch 590, Loss: 0.03056001476943493\n",
            "Epoch 1, Batch 600, Loss: 0.036173559725284576\n",
            "Epoch 1, Batch 610, Loss: 0.03880463168025017\n",
            "Epoch 1, Batch 620, Loss: 0.03283926472067833\n",
            "Epoch 1, Batch 630, Loss: 0.01929866336286068\n",
            "Epoch 1, Batch 640, Loss: 0.02451915852725506\n",
            "Epoch 1, Batch 650, Loss: 0.0388934426009655\n",
            "Epoch 1, Batch 660, Loss: 0.03760379180312157\n",
            "Epoch 1, Batch 670, Loss: 0.03272980824112892\n",
            "Epoch 1, Batch 680, Loss: 0.027593251317739487\n",
            "Epoch 1, Batch 690, Loss: 0.03280137479305267\n",
            "Epoch 1, Batch 700, Loss: 0.035641565918922424\n",
            "Epoch 1, Batch 710, Loss: 0.025611596181988716\n",
            "Epoch 1, Average Loss: 0.0007453790935876599\n",
            "Epoch 2, Batch 0, Loss: 0.023156190291047096\n",
            "Epoch 2, Batch 10, Loss: 0.029737910255789757\n",
            "Epoch 2, Batch 20, Loss: 0.02970888651907444\n",
            "Epoch 2, Batch 30, Loss: 0.02676263079047203\n",
            "Epoch 2, Batch 40, Loss: 0.023212164640426636\n",
            "Epoch 2, Batch 50, Loss: 0.038598403334617615\n",
            "Epoch 2, Batch 60, Loss: 0.022675136104226112\n",
            "Epoch 2, Batch 70, Loss: 0.021759137511253357\n",
            "Epoch 2, Batch 80, Loss: 0.041098885238170624\n",
            "Epoch 2, Batch 90, Loss: 0.02548544481396675\n",
            "Epoch 2, Batch 100, Loss: 0.027334487065672874\n",
            "Epoch 2, Batch 110, Loss: 0.02843649499118328\n",
            "Epoch 2, Batch 120, Loss: 0.02293878048658371\n",
            "Epoch 2, Batch 130, Loss: 0.02386965975165367\n",
            "Epoch 2, Batch 140, Loss: 0.026384517550468445\n",
            "Epoch 2, Batch 150, Loss: 0.022970492020249367\n",
            "Epoch 2, Batch 160, Loss: 0.03133687376976013\n",
            "Epoch 2, Batch 170, Loss: 0.026181848719716072\n",
            "Epoch 2, Batch 180, Loss: 0.03531144559383392\n",
            "Epoch 2, Batch 190, Loss: 0.023867186158895493\n",
            "Epoch 2, Batch 200, Loss: 0.024680957198143005\n",
            "Epoch 2, Batch 210, Loss: 0.03412628173828125\n",
            "Epoch 2, Batch 220, Loss: 0.032888226211071014\n",
            "Epoch 2, Batch 230, Loss: 0.02736763283610344\n",
            "Epoch 2, Batch 240, Loss: 0.02886463887989521\n",
            "Epoch 2, Batch 250, Loss: 0.028587941080331802\n",
            "Epoch 2, Batch 260, Loss: 0.01947650872170925\n",
            "Epoch 2, Batch 270, Loss: 0.02339746616780758\n",
            "Epoch 2, Batch 280, Loss: 0.018665870651602745\n",
            "Epoch 2, Batch 290, Loss: 0.026199214160442352\n",
            "Epoch 2, Batch 300, Loss: 0.024853844195604324\n",
            "Epoch 2, Batch 310, Loss: 0.027643056586384773\n",
            "Epoch 2, Batch 320, Loss: 0.023707371205091476\n",
            "Epoch 2, Batch 330, Loss: 0.030999362468719482\n",
            "Epoch 2, Batch 340, Loss: 0.01964038610458374\n",
            "Epoch 2, Batch 350, Loss: 0.03553541377186775\n",
            "Epoch 2, Batch 360, Loss: 0.027064627036452293\n",
            "Epoch 2, Batch 370, Loss: 0.03834771364927292\n",
            "Epoch 2, Batch 380, Loss: 0.02270865999162197\n",
            "Epoch 2, Batch 390, Loss: 0.03156372159719467\n",
            "Epoch 2, Batch 400, Loss: 0.0211627334356308\n",
            "Epoch 2, Batch 410, Loss: 0.020099610090255737\n",
            "Epoch 2, Batch 420, Loss: 0.02122231386601925\n",
            "Epoch 2, Batch 430, Loss: 0.01809484325349331\n",
            "Epoch 2, Batch 440, Loss: 0.01871633529663086\n",
            "Epoch 2, Batch 450, Loss: 0.024052105844020844\n",
            "Epoch 2, Batch 460, Loss: 0.023133903741836548\n",
            "Epoch 2, Batch 470, Loss: 0.021222621202468872\n",
            "Epoch 2, Batch 480, Loss: 0.03846246749162674\n",
            "Epoch 2, Batch 490, Loss: 0.020191598683595657\n",
            "Epoch 2, Batch 500, Loss: 0.026481444016098976\n",
            "Epoch 2, Batch 510, Loss: 0.02858402207493782\n",
            "Epoch 2, Batch 520, Loss: 0.03028889372944832\n",
            "Epoch 2, Batch 530, Loss: 0.027175139635801315\n",
            "Epoch 2, Batch 540, Loss: 0.020731601864099503\n",
            "Epoch 2, Batch 550, Loss: 0.027405109256505966\n",
            "Epoch 2, Batch 560, Loss: 0.019331788644194603\n",
            "Epoch 2, Batch 570, Loss: 0.01971786841750145\n",
            "Epoch 2, Batch 580, Loss: 0.020854227244853973\n",
            "Epoch 2, Batch 590, Loss: 0.03400096669793129\n",
            "Epoch 2, Batch 600, Loss: 0.028564926236867905\n",
            "Epoch 2, Batch 610, Loss: 0.023587815463542938\n",
            "Epoch 2, Batch 620, Loss: 0.035834185779094696\n",
            "Epoch 2, Batch 630, Loss: 0.031010081991553307\n",
            "Epoch 2, Batch 640, Loss: 0.03108496218919754\n",
            "Epoch 2, Batch 650, Loss: 0.02273387834429741\n",
            "Epoch 2, Batch 660, Loss: 0.02576116845011711\n",
            "Epoch 2, Batch 670, Loss: 0.024689359590411186\n",
            "Epoch 2, Batch 680, Loss: 0.02909340150654316\n",
            "Epoch 2, Batch 690, Loss: 0.02340703457593918\n",
            "Epoch 2, Batch 700, Loss: 0.024798912927508354\n",
            "Epoch 2, Batch 710, Loss: 0.029751023277640343\n",
            "Epoch 2, Average Loss: 0.0004272738248240501\n",
            "Epoch 3, Batch 0, Loss: 0.024351848289370537\n",
            "Epoch 3, Batch 10, Loss: 0.029004767537117004\n",
            "Epoch 3, Batch 20, Loss: 0.036780331283807755\n",
            "Epoch 3, Batch 30, Loss: 0.025996917858719826\n",
            "Epoch 3, Batch 40, Loss: 0.02817608416080475\n",
            "Epoch 3, Batch 50, Loss: 0.02196018025279045\n",
            "Epoch 3, Batch 60, Loss: 0.02133115939795971\n",
            "Epoch 3, Batch 70, Loss: 0.02586216665804386\n",
            "Epoch 3, Batch 80, Loss: 0.026616916060447693\n",
            "Epoch 3, Batch 90, Loss: 0.021057549864053726\n",
            "Epoch 3, Batch 100, Loss: 0.027400843799114227\n",
            "Epoch 3, Batch 110, Loss: 0.024966197088360786\n",
            "Epoch 3, Batch 120, Loss: 0.021880023181438446\n",
            "Epoch 3, Batch 130, Loss: 0.0186622217297554\n",
            "Epoch 3, Batch 140, Loss: 0.03381031006574631\n",
            "Epoch 3, Batch 150, Loss: 0.024257181212306023\n",
            "Epoch 3, Batch 160, Loss: 0.03229893743991852\n",
            "Epoch 3, Batch 170, Loss: 0.030156776309013367\n",
            "Epoch 3, Batch 180, Loss: 0.022239897400140762\n",
            "Epoch 3, Batch 190, Loss: 0.020181359723210335\n",
            "Epoch 3, Batch 200, Loss: 0.025802534073591232\n",
            "Epoch 3, Batch 210, Loss: 0.02828098274767399\n",
            "Epoch 3, Batch 220, Loss: 0.027921898290514946\n",
            "Epoch 3, Batch 230, Loss: 0.021376874297857285\n",
            "Epoch 3, Batch 240, Loss: 0.02067217417061329\n",
            "Epoch 3, Batch 250, Loss: 0.03013007342815399\n",
            "Epoch 3, Batch 260, Loss: 0.030338771641254425\n",
            "Epoch 3, Batch 270, Loss: 0.024800803512334824\n",
            "Epoch 3, Batch 280, Loss: 0.023480651900172234\n",
            "Epoch 3, Batch 290, Loss: 0.022100962698459625\n",
            "Epoch 3, Batch 300, Loss: 0.018519068136811256\n",
            "Epoch 3, Batch 310, Loss: 0.02574165351688862\n",
            "Epoch 3, Batch 320, Loss: 0.025605423375964165\n",
            "Epoch 3, Batch 330, Loss: 0.02521245740354061\n",
            "Epoch 3, Batch 340, Loss: 0.021263042464852333\n",
            "Epoch 3, Batch 350, Loss: 0.023570451885461807\n",
            "Epoch 3, Batch 360, Loss: 0.02503587305545807\n",
            "Epoch 3, Batch 370, Loss: 0.02000056393444538\n",
            "Epoch 3, Batch 380, Loss: 0.034070562571287155\n",
            "Epoch 3, Batch 390, Loss: 0.02498560957610607\n",
            "Epoch 3, Batch 400, Loss: 0.024835458025336266\n",
            "Epoch 3, Batch 410, Loss: 0.04114075377583504\n",
            "Epoch 3, Batch 420, Loss: 0.020673347637057304\n",
            "Epoch 3, Batch 430, Loss: 0.03435388207435608\n",
            "Epoch 3, Batch 440, Loss: 0.027336057275533676\n",
            "Epoch 3, Batch 450, Loss: 0.021948589012026787\n",
            "Epoch 3, Batch 460, Loss: 0.02460571564733982\n",
            "Epoch 3, Batch 470, Loss: 0.03492220491170883\n",
            "Epoch 3, Batch 480, Loss: 0.021554473787546158\n",
            "Epoch 3, Batch 490, Loss: 0.019764909520745277\n",
            "Epoch 3, Batch 500, Loss: 0.021439997479319572\n",
            "Epoch 3, Batch 510, Loss: 0.031012695282697678\n",
            "Epoch 3, Batch 520, Loss: 0.022594638168811798\n",
            "Epoch 3, Batch 530, Loss: 0.021468866616487503\n",
            "Epoch 3, Batch 540, Loss: 0.028809018433094025\n",
            "Epoch 3, Batch 550, Loss: 0.023787612095475197\n",
            "Epoch 3, Batch 560, Loss: 0.03717054799199104\n",
            "Epoch 3, Batch 570, Loss: 0.028128942474722862\n",
            "Epoch 3, Batch 580, Loss: 0.042374543845653534\n",
            "Epoch 3, Batch 590, Loss: 0.02619255520403385\n",
            "Epoch 3, Batch 600, Loss: 0.02098390832543373\n",
            "Epoch 3, Batch 610, Loss: 0.024650009348988533\n",
            "Epoch 3, Batch 620, Loss: 0.025006558746099472\n",
            "Epoch 3, Batch 630, Loss: 0.02138904668390751\n",
            "Epoch 3, Batch 640, Loss: 0.021257828921079636\n",
            "Epoch 3, Batch 650, Loss: 0.028956592082977295\n",
            "Epoch 3, Batch 660, Loss: 0.024554412811994553\n",
            "Epoch 3, Batch 670, Loss: 0.03232395648956299\n",
            "Epoch 3, Batch 680, Loss: 0.029755011200904846\n",
            "Epoch 3, Batch 690, Loss: 0.020226281136274338\n",
            "Epoch 3, Batch 700, Loss: 0.02034188061952591\n",
            "Epoch 3, Batch 710, Loss: 0.017919445410370827\n",
            "Epoch 3, Average Loss: 0.000397166982762772\n",
            "Epoch 4, Batch 0, Loss: 0.0269706342369318\n",
            "Epoch 4, Batch 10, Loss: 0.023625316098332405\n",
            "Epoch 4, Batch 20, Loss: 0.03607218340039253\n",
            "Epoch 4, Batch 30, Loss: 0.01917268894612789\n",
            "Epoch 4, Batch 40, Loss: 0.019916046410799026\n",
            "Epoch 4, Batch 50, Loss: 0.020285796374082565\n",
            "Epoch 4, Batch 60, Loss: 0.026237372308969498\n",
            "Epoch 4, Batch 70, Loss: 0.023908644914627075\n",
            "Epoch 4, Batch 80, Loss: 0.028136692941188812\n",
            "Epoch 4, Batch 90, Loss: 0.025592895224690437\n",
            "Epoch 4, Batch 100, Loss: 0.027752812951803207\n",
            "Epoch 4, Batch 110, Loss: 0.034262724220752716\n",
            "Epoch 4, Batch 120, Loss: 0.022850535809993744\n",
            "Epoch 4, Batch 130, Loss: 0.028569690883159637\n",
            "Epoch 4, Batch 140, Loss: 0.024453066289424896\n",
            "Epoch 4, Batch 150, Loss: 0.027996214106678963\n",
            "Epoch 4, Batch 160, Loss: 0.025321677327156067\n",
            "Epoch 4, Batch 170, Loss: 0.025684649124741554\n",
            "Epoch 4, Batch 180, Loss: 0.02096625603735447\n",
            "Epoch 4, Batch 190, Loss: 0.022982478141784668\n",
            "Epoch 4, Batch 200, Loss: 0.025380399078130722\n",
            "Epoch 4, Batch 210, Loss: 0.027630724012851715\n",
            "Epoch 4, Batch 220, Loss: 0.01922479085624218\n",
            "Epoch 4, Batch 230, Loss: 0.024667158722877502\n",
            "Epoch 4, Batch 240, Loss: 0.019828807562589645\n",
            "Epoch 4, Batch 250, Loss: 0.025687959045171738\n",
            "Epoch 4, Batch 260, Loss: 0.02579185739159584\n",
            "Epoch 4, Batch 270, Loss: 0.038639623671770096\n",
            "Epoch 4, Batch 280, Loss: 0.02735108882188797\n",
            "Epoch 4, Batch 290, Loss: 0.015755988657474518\n",
            "Epoch 4, Batch 300, Loss: 0.0239948108792305\n",
            "Epoch 4, Batch 310, Loss: 0.020975938066840172\n",
            "Epoch 4, Batch 320, Loss: 0.022613996639847755\n",
            "Epoch 4, Batch 330, Loss: 0.034309662878513336\n",
            "Epoch 4, Batch 340, Loss: 0.02051728591322899\n",
            "Epoch 4, Batch 350, Loss: 0.029430698603391647\n",
            "Epoch 4, Batch 360, Loss: 0.020648784935474396\n",
            "Epoch 4, Batch 370, Loss: 0.025912510231137276\n",
            "Epoch 4, Batch 380, Loss: 0.0227506086230278\n",
            "Epoch 4, Batch 390, Loss: 0.020925182849168777\n",
            "Epoch 4, Batch 400, Loss: 0.02365371212363243\n",
            "Epoch 4, Batch 410, Loss: 0.02968405745923519\n",
            "Epoch 4, Batch 420, Loss: 0.023462116718292236\n",
            "Epoch 4, Batch 430, Loss: 0.02598053216934204\n",
            "Epoch 4, Batch 440, Loss: 0.017202693969011307\n",
            "Epoch 4, Batch 450, Loss: 0.01753275841474533\n",
            "Epoch 4, Batch 460, Loss: 0.017062613740563393\n",
            "Epoch 4, Batch 470, Loss: 0.025893397629261017\n",
            "Epoch 4, Batch 480, Loss: 0.016791503876447678\n",
            "Epoch 4, Batch 490, Loss: 0.021380793303251266\n",
            "Epoch 4, Batch 500, Loss: 0.023036949336528778\n",
            "Epoch 4, Batch 510, Loss: 0.013881858438253403\n",
            "Epoch 4, Batch 520, Loss: 0.019108029082417488\n",
            "Epoch 4, Batch 530, Loss: 0.02044554613530636\n",
            "Epoch 4, Batch 540, Loss: 0.020616445690393448\n",
            "Epoch 4, Batch 550, Loss: 0.021976279094815254\n",
            "Epoch 4, Batch 560, Loss: 0.029677491635084152\n",
            "Epoch 4, Batch 570, Loss: 0.027680061757564545\n",
            "Epoch 4, Batch 580, Loss: 0.01909918710589409\n",
            "Epoch 4, Batch 590, Loss: 0.01927296631038189\n",
            "Epoch 4, Batch 600, Loss: 0.02301916852593422\n",
            "Epoch 4, Batch 610, Loss: 0.02880984917283058\n",
            "Epoch 4, Batch 620, Loss: 0.018319252878427505\n",
            "Epoch 4, Batch 630, Loss: 0.019066613167524338\n",
            "Epoch 4, Batch 640, Loss: 0.027321260422468185\n",
            "Epoch 4, Batch 650, Loss: 0.03089110553264618\n",
            "Epoch 4, Batch 660, Loss: 0.0224029291421175\n",
            "Epoch 4, Batch 670, Loss: 0.022864798083901405\n",
            "Epoch 4, Batch 680, Loss: 0.019267963245511055\n",
            "Epoch 4, Batch 690, Loss: 0.019600488245487213\n",
            "Epoch 4, Batch 700, Loss: 0.019252834841609\n",
            "Epoch 4, Batch 710, Loss: 0.01959451474249363\n",
            "Epoch 4, Average Loss: 0.0003823192381152809\n",
            "Epoch 5, Batch 0, Loss: 0.019426338374614716\n",
            "Epoch 5, Batch 10, Loss: 0.029239144176244736\n",
            "Epoch 5, Batch 20, Loss: 0.03200692683458328\n",
            "Epoch 5, Batch 30, Loss: 0.020301250740885735\n",
            "Epoch 5, Batch 40, Loss: 0.029510322958230972\n",
            "Epoch 5, Batch 50, Loss: 0.02152162976562977\n",
            "Epoch 5, Batch 60, Loss: 0.021483592689037323\n",
            "Epoch 5, Batch 70, Loss: 0.024592669680714607\n",
            "Epoch 5, Batch 80, Loss: 0.020928924903273582\n",
            "Epoch 5, Batch 90, Loss: 0.023528870195150375\n",
            "Epoch 5, Batch 100, Loss: 0.024166710674762726\n",
            "Epoch 5, Batch 110, Loss: 0.020954448729753494\n",
            "Epoch 5, Batch 120, Loss: 0.01831868104636669\n",
            "Epoch 5, Batch 130, Loss: 0.01956883817911148\n",
            "Epoch 5, Batch 140, Loss: 0.02134433202445507\n",
            "Epoch 5, Batch 150, Loss: 0.026391778141260147\n",
            "Epoch 5, Batch 160, Loss: 0.016894908621907234\n",
            "Epoch 5, Batch 170, Loss: 0.020605776458978653\n",
            "Epoch 5, Batch 180, Loss: 0.01759529672563076\n",
            "Epoch 5, Batch 190, Loss: 0.01704196259379387\n",
            "Epoch 5, Batch 200, Loss: 0.02519061230123043\n",
            "Epoch 5, Batch 210, Loss: 0.028352094814181328\n",
            "Epoch 5, Batch 220, Loss: 0.0254153311252594\n",
            "Epoch 5, Batch 230, Loss: 0.01968196965754032\n",
            "Epoch 5, Batch 240, Loss: 0.02157580852508545\n",
            "Epoch 5, Batch 250, Loss: 0.021892119199037552\n",
            "Epoch 5, Batch 260, Loss: 0.025445058941841125\n",
            "Epoch 5, Batch 270, Loss: 0.022231819108128548\n",
            "Epoch 5, Batch 280, Loss: 0.0297378059476614\n",
            "Epoch 5, Batch 290, Loss: 0.027279581874608994\n",
            "Epoch 5, Batch 300, Loss: 0.024074137210845947\n",
            "Epoch 5, Batch 310, Loss: 0.01797434315085411\n",
            "Epoch 5, Batch 320, Loss: 0.025941940024495125\n",
            "Epoch 5, Batch 330, Loss: 0.031461045145988464\n",
            "Epoch 5, Batch 340, Loss: 0.022030407562851906\n",
            "Epoch 5, Batch 350, Loss: 0.03485307842493057\n",
            "Epoch 5, Batch 360, Loss: 0.022727767005562782\n",
            "Epoch 5, Batch 370, Loss: 0.019995510578155518\n",
            "Epoch 5, Batch 380, Loss: 0.029275471344590187\n",
            "Epoch 5, Batch 390, Loss: 0.019119512289762497\n",
            "Epoch 5, Batch 400, Loss: 0.02339651808142662\n",
            "Epoch 5, Batch 410, Loss: 0.027698274701833725\n",
            "Epoch 5, Batch 420, Loss: 0.021920429542660713\n",
            "Epoch 5, Batch 430, Loss: 0.02229870855808258\n",
            "Epoch 5, Batch 440, Loss: 0.022388439625501633\n",
            "Epoch 5, Batch 450, Loss: 0.020985091105103493\n",
            "Epoch 5, Batch 460, Loss: 0.022099513560533524\n",
            "Epoch 5, Batch 470, Loss: 0.01819855161011219\n",
            "Epoch 5, Batch 480, Loss: 0.016165876761078835\n",
            "Epoch 5, Batch 490, Loss: 0.02156883105635643\n",
            "Epoch 5, Batch 500, Loss: 0.022055286914110184\n",
            "Epoch 5, Batch 510, Loss: 0.027195347473025322\n",
            "Epoch 5, Batch 520, Loss: 0.02715126983821392\n",
            "Epoch 5, Batch 530, Loss: 0.020643681287765503\n",
            "Epoch 5, Batch 540, Loss: 0.018784988671541214\n",
            "Epoch 5, Batch 550, Loss: 0.038689546287059784\n",
            "Epoch 5, Batch 560, Loss: 0.02896682173013687\n",
            "Epoch 5, Batch 570, Loss: 0.030876465141773224\n",
            "Epoch 5, Batch 580, Loss: 0.027820106595754623\n",
            "Epoch 5, Batch 590, Loss: 0.024311980232596397\n",
            "Epoch 5, Batch 600, Loss: 0.02504061907529831\n",
            "Epoch 5, Batch 610, Loss: 0.02246798202395439\n",
            "Epoch 5, Batch 620, Loss: 0.022394338622689247\n",
            "Epoch 5, Batch 630, Loss: 0.016656365245580673\n",
            "Epoch 5, Batch 640, Loss: 0.020899184048175812\n",
            "Epoch 5, Batch 650, Loss: 0.03068609908223152\n",
            "Epoch 5, Batch 660, Loss: 0.02385205775499344\n",
            "Epoch 5, Batch 670, Loss: 0.017827589064836502\n",
            "Epoch 5, Batch 680, Loss: 0.02692856639623642\n",
            "Epoch 5, Batch 690, Loss: 0.02496924437582493\n",
            "Epoch 5, Batch 700, Loss: 0.022488586604595184\n",
            "Epoch 5, Batch 710, Loss: 0.0271714199334383\n",
            "Epoch 5, Average Loss: 0.000366414785491312\n",
            "Epoch 6, Batch 0, Loss: 0.022632542997598648\n",
            "Epoch 6, Batch 10, Loss: 0.028036536648869514\n",
            "Epoch 6, Batch 20, Loss: 0.02270273119211197\n",
            "Epoch 6, Batch 30, Loss: 0.03410691022872925\n",
            "Epoch 6, Batch 40, Loss: 0.02921394258737564\n",
            "Epoch 6, Batch 50, Loss: 0.01730743609368801\n",
            "Epoch 6, Batch 60, Loss: 0.02177729830145836\n",
            "Epoch 6, Batch 70, Loss: 0.018113821744918823\n",
            "Epoch 6, Batch 80, Loss: 0.02629239112138748\n",
            "Epoch 6, Batch 90, Loss: 0.03164045140147209\n",
            "Epoch 6, Batch 100, Loss: 0.02497091330587864\n",
            "Epoch 6, Batch 110, Loss: 0.02420375496149063\n",
            "Epoch 6, Batch 120, Loss: 0.027451200410723686\n",
            "Epoch 6, Batch 130, Loss: 0.024704935029149055\n",
            "Epoch 6, Batch 140, Loss: 0.020266737788915634\n",
            "Epoch 6, Batch 150, Loss: 0.01609067991375923\n",
            "Epoch 6, Batch 160, Loss: 0.017718445509672165\n",
            "Epoch 6, Batch 170, Loss: 0.03281122073531151\n",
            "Epoch 6, Batch 180, Loss: 0.022943895310163498\n",
            "Epoch 6, Batch 190, Loss: 0.022806765511631966\n",
            "Epoch 6, Batch 200, Loss: 0.024575168266892433\n",
            "Epoch 6, Batch 210, Loss: 0.02012464590370655\n",
            "Epoch 6, Batch 220, Loss: 0.021045953035354614\n",
            "Epoch 6, Batch 230, Loss: 0.025564774870872498\n",
            "Epoch 6, Batch 240, Loss: 0.0334697887301445\n",
            "Epoch 6, Batch 250, Loss: 0.02072203904390335\n",
            "Epoch 6, Batch 260, Loss: 0.026042291894555092\n",
            "Epoch 6, Batch 270, Loss: 0.02146116830408573\n",
            "Epoch 6, Batch 280, Loss: 0.016556700691580772\n",
            "Epoch 6, Batch 290, Loss: 0.01515633799135685\n",
            "Epoch 6, Batch 300, Loss: 0.023508071899414062\n",
            "Epoch 6, Batch 310, Loss: 0.017908109351992607\n",
            "Epoch 6, Batch 320, Loss: 0.028054671362042427\n",
            "Epoch 6, Batch 330, Loss: 0.022462818771600723\n",
            "Epoch 6, Batch 340, Loss: 0.01704566925764084\n",
            "Epoch 6, Batch 350, Loss: 0.015654748305678368\n",
            "Epoch 6, Batch 360, Loss: 0.019139543175697327\n",
            "Epoch 6, Batch 370, Loss: 0.015689531341195107\n",
            "Epoch 6, Batch 380, Loss: 0.016796283423900604\n",
            "Epoch 6, Batch 390, Loss: 0.017380379140377045\n",
            "Epoch 6, Batch 400, Loss: 0.02774238772690296\n",
            "Epoch 6, Batch 410, Loss: 0.024886498227715492\n",
            "Epoch 6, Batch 420, Loss: 0.01874431222677231\n",
            "Epoch 6, Batch 430, Loss: 0.027621664106845856\n",
            "Epoch 6, Batch 440, Loss: 0.018742447718977928\n",
            "Epoch 6, Batch 450, Loss: 0.02054337039589882\n",
            "Epoch 6, Batch 460, Loss: 0.020840555429458618\n",
            "Epoch 6, Batch 470, Loss: 0.017304839566349983\n",
            "Epoch 6, Batch 480, Loss: 0.025255633518099785\n",
            "Epoch 6, Batch 490, Loss: 0.025469258427619934\n",
            "Epoch 6, Batch 500, Loss: 0.02182169444859028\n",
            "Epoch 6, Batch 510, Loss: 0.012668628245592117\n",
            "Epoch 6, Batch 520, Loss: 0.0235757939517498\n",
            "Epoch 6, Batch 530, Loss: 0.01999785006046295\n",
            "Epoch 6, Batch 540, Loss: 0.02110515534877777\n",
            "Epoch 6, Batch 550, Loss: 0.03572758287191391\n",
            "Epoch 6, Batch 560, Loss: 0.016995372250676155\n",
            "Epoch 6, Batch 570, Loss: 0.02908211573958397\n",
            "Epoch 6, Batch 580, Loss: 0.02144562639296055\n",
            "Epoch 6, Batch 590, Loss: 0.01749997027218342\n",
            "Epoch 6, Batch 600, Loss: 0.025736646726727486\n",
            "Epoch 6, Batch 610, Loss: 0.01910916157066822\n",
            "Epoch 6, Batch 620, Loss: 0.0407453253865242\n",
            "Epoch 6, Batch 630, Loss: 0.01435188576579094\n",
            "Epoch 6, Batch 640, Loss: 0.021124333143234253\n",
            "Epoch 6, Batch 650, Loss: 0.014853817410767078\n",
            "Epoch 6, Batch 660, Loss: 0.018000653013586998\n",
            "Epoch 6, Batch 670, Loss: 0.012143845669925213\n",
            "Epoch 6, Batch 680, Loss: 0.022760864347219467\n",
            "Epoch 6, Batch 690, Loss: 0.02009003981947899\n",
            "Epoch 6, Batch 700, Loss: 0.020668629556894302\n",
            "Epoch 6, Batch 710, Loss: 0.025510333478450775\n",
            "Epoch 6, Average Loss: 0.00035036837404834367\n",
            "Epoch 7, Batch 0, Loss: 0.02437105029821396\n",
            "Epoch 7, Batch 10, Loss: 0.028743436560034752\n",
            "Epoch 7, Batch 20, Loss: 0.012608018703758717\n",
            "Epoch 7, Batch 30, Loss: 0.017790786921977997\n",
            "Epoch 7, Batch 40, Loss: 0.025734568014740944\n",
            "Epoch 7, Batch 50, Loss: 0.02128380909562111\n",
            "Epoch 7, Batch 60, Loss: 0.02042585425078869\n",
            "Epoch 7, Batch 70, Loss: 0.030131174251437187\n",
            "Epoch 7, Batch 80, Loss: 0.01949106715619564\n",
            "Epoch 7, Batch 90, Loss: 0.01606031134724617\n",
            "Epoch 7, Batch 100, Loss: 0.013326489366590977\n",
            "Epoch 7, Batch 110, Loss: 0.02455233596265316\n",
            "Epoch 7, Batch 120, Loss: 0.03098003752529621\n",
            "Epoch 7, Batch 130, Loss: 0.02234928496181965\n",
            "Epoch 7, Batch 140, Loss: 0.021395988762378693\n",
            "Epoch 7, Batch 150, Loss: 0.014218855649232864\n",
            "Epoch 7, Batch 160, Loss: 0.01976836286485195\n",
            "Epoch 7, Batch 170, Loss: 0.024017393589019775\n",
            "Epoch 7, Batch 180, Loss: 0.01911449246108532\n",
            "Epoch 7, Batch 190, Loss: 0.021587993949651718\n",
            "Epoch 7, Batch 200, Loss: 0.022040024399757385\n",
            "Epoch 7, Batch 210, Loss: 0.0334550105035305\n",
            "Epoch 7, Batch 220, Loss: 0.017209751531481743\n",
            "Epoch 7, Batch 230, Loss: 0.022019026800990105\n",
            "Epoch 7, Batch 240, Loss: 0.018514102324843407\n",
            "Epoch 7, Batch 250, Loss: 0.016861915588378906\n",
            "Epoch 7, Batch 260, Loss: 0.023665033280849457\n",
            "Epoch 7, Batch 270, Loss: 0.016866540536284447\n",
            "Epoch 7, Batch 280, Loss: 0.017393482849001884\n",
            "Epoch 7, Batch 290, Loss: 0.016800615936517715\n",
            "Epoch 7, Batch 300, Loss: 0.01785661280155182\n",
            "Epoch 7, Batch 310, Loss: 0.027776164934039116\n",
            "Epoch 7, Batch 320, Loss: 0.017135674133896828\n",
            "Epoch 7, Batch 330, Loss: 0.015277773141860962\n",
            "Epoch 7, Batch 340, Loss: 0.018505973741412163\n",
            "Epoch 7, Batch 350, Loss: 0.023434225469827652\n",
            "Epoch 7, Batch 360, Loss: 0.026837052777409554\n",
            "Epoch 7, Batch 370, Loss: 0.02297522872686386\n",
            "Epoch 7, Batch 380, Loss: 0.01439264602959156\n",
            "Epoch 7, Batch 390, Loss: 0.020848484709858894\n",
            "Epoch 7, Batch 400, Loss: 0.01750919036567211\n",
            "Epoch 7, Batch 410, Loss: 0.021833892911672592\n",
            "Epoch 7, Batch 420, Loss: 0.020192693918943405\n",
            "Epoch 7, Batch 430, Loss: 0.026086177676916122\n",
            "Epoch 7, Batch 440, Loss: 0.0201170165091753\n",
            "Epoch 7, Batch 450, Loss: 0.015030821785330772\n",
            "Epoch 7, Batch 460, Loss: 0.02179313823580742\n",
            "Epoch 7, Batch 470, Loss: 0.021044965833425522\n",
            "Epoch 7, Batch 480, Loss: 0.02189534157514572\n",
            "Epoch 7, Batch 490, Loss: 0.027823368087410927\n",
            "Epoch 7, Batch 500, Loss: 0.01228000782430172\n",
            "Epoch 7, Batch 510, Loss: 0.02666408196091652\n",
            "Epoch 7, Batch 520, Loss: 0.02608533576130867\n",
            "Epoch 7, Batch 530, Loss: 0.01899796724319458\n",
            "Epoch 7, Batch 540, Loss: 0.015832161530852318\n",
            "Epoch 7, Batch 550, Loss: 0.02242600917816162\n",
            "Epoch 7, Batch 560, Loss: 0.021103303879499435\n",
            "Epoch 7, Batch 570, Loss: 0.021151460707187653\n",
            "Epoch 7, Batch 580, Loss: 0.01566469483077526\n",
            "Epoch 7, Batch 590, Loss: 0.02049853466451168\n",
            "Epoch 7, Batch 600, Loss: 0.02344535104930401\n",
            "Epoch 7, Batch 610, Loss: 0.02480379492044449\n",
            "Epoch 7, Batch 620, Loss: 0.021871639415621758\n",
            "Epoch 7, Batch 630, Loss: 0.025517772883176804\n",
            "Epoch 7, Batch 640, Loss: 0.02150513045489788\n",
            "Epoch 7, Batch 650, Loss: 0.018259231001138687\n",
            "Epoch 7, Batch 660, Loss: 0.027543704956769943\n",
            "Epoch 7, Batch 670, Loss: 0.01819824054837227\n",
            "Epoch 7, Batch 680, Loss: 0.01814640499651432\n",
            "Epoch 7, Batch 690, Loss: 0.019161518663167953\n",
            "Epoch 7, Batch 700, Loss: 0.01908600516617298\n",
            "Epoch 7, Batch 710, Loss: 0.02449199929833412\n",
            "Epoch 7, Average Loss: 0.00033291614726937226\n",
            "Epoch 8, Batch 0, Loss: 0.02633453905582428\n",
            "Epoch 8, Batch 10, Loss: 0.01573784276843071\n",
            "Epoch 8, Batch 20, Loss: 0.01845572330057621\n",
            "Epoch 8, Batch 30, Loss: 0.027442922815680504\n",
            "Epoch 8, Batch 40, Loss: 0.01566188596189022\n",
            "Epoch 8, Batch 50, Loss: 0.022121567279100418\n",
            "Epoch 8, Batch 60, Loss: 0.018143093213438988\n",
            "Epoch 8, Batch 70, Loss: 0.014431865885853767\n",
            "Epoch 8, Batch 80, Loss: 0.02417699806392193\n",
            "Epoch 8, Batch 90, Loss: 0.016377437859773636\n",
            "Epoch 8, Batch 100, Loss: 0.024775903671979904\n",
            "Epoch 8, Batch 110, Loss: 0.023176630958914757\n",
            "Epoch 8, Batch 120, Loss: 0.016649356111884117\n",
            "Epoch 8, Batch 130, Loss: 0.015786737203598022\n",
            "Epoch 8, Batch 140, Loss: 0.02535102516412735\n",
            "Epoch 8, Batch 150, Loss: 0.017948821187019348\n",
            "Epoch 8, Batch 160, Loss: 0.02343090996146202\n",
            "Epoch 8, Batch 170, Loss: 0.01646684855222702\n",
            "Epoch 8, Batch 180, Loss: 0.02332700788974762\n",
            "Epoch 8, Batch 190, Loss: 0.01935293897986412\n",
            "Epoch 8, Batch 200, Loss: 0.02810269594192505\n",
            "Epoch 8, Batch 210, Loss: 0.027839012444019318\n",
            "Epoch 8, Batch 220, Loss: 0.020837079733610153\n",
            "Epoch 8, Batch 230, Loss: 0.028219476342201233\n",
            "Epoch 8, Batch 240, Loss: 0.016438888385891914\n",
            "Epoch 8, Batch 250, Loss: 0.02319354936480522\n",
            "Epoch 8, Batch 260, Loss: 0.024846293032169342\n",
            "Epoch 8, Batch 270, Loss: 0.01787717081606388\n",
            "Epoch 8, Batch 280, Loss: 0.021316764876246452\n",
            "Epoch 8, Batch 290, Loss: 0.023749858140945435\n",
            "Epoch 8, Batch 300, Loss: 0.0130342161282897\n",
            "Epoch 8, Batch 310, Loss: 0.01837683841586113\n",
            "Epoch 8, Batch 320, Loss: 0.014885674230754375\n",
            "Epoch 8, Batch 330, Loss: 0.017687566578388214\n",
            "Epoch 8, Batch 340, Loss: 0.015932854264974594\n",
            "Epoch 8, Batch 350, Loss: 0.01738288626074791\n",
            "Epoch 8, Batch 360, Loss: 0.024780485779047012\n",
            "Epoch 8, Batch 370, Loss: 0.02180459350347519\n",
            "Epoch 8, Batch 380, Loss: 0.029218897223472595\n",
            "Epoch 8, Batch 390, Loss: 0.015105151571333408\n",
            "Epoch 8, Batch 400, Loss: 0.018236922100186348\n",
            "Epoch 8, Batch 410, Loss: 0.018652981147170067\n",
            "Epoch 8, Batch 420, Loss: 0.01379357185214758\n",
            "Epoch 8, Batch 430, Loss: 0.017680026590824127\n",
            "Epoch 8, Batch 440, Loss: 0.021710632368922234\n",
            "Epoch 8, Batch 450, Loss: 0.026707429438829422\n",
            "Epoch 8, Batch 460, Loss: 0.014069488272070885\n",
            "Epoch 8, Batch 470, Loss: 0.01773015782237053\n",
            "Epoch 8, Batch 480, Loss: 0.017910020425915718\n",
            "Epoch 8, Batch 490, Loss: 0.019794374704360962\n",
            "Epoch 8, Batch 500, Loss: 0.0183897465467453\n",
            "Epoch 8, Batch 510, Loss: 0.01895793527364731\n",
            "Epoch 8, Batch 520, Loss: 0.019449595361948013\n",
            "Epoch 8, Batch 530, Loss: 0.016864275559782982\n",
            "Epoch 8, Batch 540, Loss: 0.015696831047534943\n",
            "Epoch 8, Batch 550, Loss: 0.01786786876618862\n",
            "Epoch 8, Batch 560, Loss: 0.02080855891108513\n",
            "Epoch 8, Batch 570, Loss: 0.019437437877058983\n",
            "Epoch 8, Batch 580, Loss: 0.022749081254005432\n",
            "Epoch 8, Batch 590, Loss: 0.02689366415143013\n",
            "Epoch 8, Batch 600, Loss: 0.015377135947346687\n",
            "Epoch 8, Batch 610, Loss: 0.015645679086446762\n",
            "Epoch 8, Batch 620, Loss: 0.017630400136113167\n",
            "Epoch 8, Batch 630, Loss: 0.026711218059062958\n",
            "Epoch 8, Batch 640, Loss: 0.019445840269327164\n",
            "Epoch 8, Batch 650, Loss: 0.022257111966609955\n",
            "Epoch 8, Batch 660, Loss: 0.020757881924510002\n",
            "Epoch 8, Batch 670, Loss: 0.015617145225405693\n",
            "Epoch 8, Batch 680, Loss: 0.019489508122205734\n",
            "Epoch 8, Batch 690, Loss: 0.02165922522544861\n",
            "Epoch 8, Batch 700, Loss: 0.017914993688464165\n",
            "Epoch 8, Batch 710, Loss: 0.015796178951859474\n",
            "Epoch 8, Average Loss: 0.000311135904031596\n",
            "Epoch 9, Batch 0, Loss: 0.023446423932909966\n",
            "Epoch 9, Batch 10, Loss: 0.01923164166510105\n",
            "Epoch 9, Batch 20, Loss: 0.018008887767791748\n",
            "Epoch 9, Batch 30, Loss: 0.014513706788420677\n",
            "Epoch 9, Batch 40, Loss: 0.021053597331047058\n",
            "Epoch 9, Batch 50, Loss: 0.018804512917995453\n",
            "Epoch 9, Batch 60, Loss: 0.016087986528873444\n",
            "Epoch 9, Batch 70, Loss: 0.0112995570525527\n",
            "Epoch 9, Batch 80, Loss: 0.027699699625372887\n",
            "Epoch 9, Batch 90, Loss: 0.01685541309416294\n",
            "Epoch 9, Batch 100, Loss: 0.02138352580368519\n",
            "Epoch 9, Batch 110, Loss: 0.016594167798757553\n",
            "Epoch 9, Batch 120, Loss: 0.0179419107735157\n",
            "Epoch 9, Batch 130, Loss: 0.015726137906312943\n",
            "Epoch 9, Batch 140, Loss: 0.020932745188474655\n",
            "Epoch 9, Batch 150, Loss: 0.023013997822999954\n",
            "Epoch 9, Batch 160, Loss: 0.014487603679299355\n",
            "Epoch 9, Batch 170, Loss: 0.022474581375718117\n",
            "Epoch 9, Batch 180, Loss: 0.01570124365389347\n",
            "Epoch 9, Batch 190, Loss: 0.013558076694607735\n",
            "Epoch 9, Batch 200, Loss: 0.018883133307099342\n",
            "Epoch 9, Batch 210, Loss: 0.012783963233232498\n",
            "Epoch 9, Batch 220, Loss: 0.01937801018357277\n",
            "Epoch 9, Batch 230, Loss: 0.027298549190163612\n",
            "Epoch 9, Batch 240, Loss: 0.016558341681957245\n",
            "Epoch 9, Batch 250, Loss: 0.01942574791610241\n",
            "Epoch 9, Batch 260, Loss: 0.015093421563506126\n",
            "Epoch 9, Batch 270, Loss: 0.015446602366864681\n",
            "Epoch 9, Batch 280, Loss: 0.013383223675191402\n",
            "Epoch 9, Batch 290, Loss: 0.015889611095190048\n",
            "Epoch 9, Batch 300, Loss: 0.01639007404446602\n",
            "Epoch 9, Batch 310, Loss: 0.022108525037765503\n",
            "Epoch 9, Batch 320, Loss: 0.029289942234754562\n",
            "Epoch 9, Batch 330, Loss: 0.015640171244740486\n",
            "Epoch 9, Batch 340, Loss: 0.018233079463243484\n",
            "Epoch 9, Batch 350, Loss: 0.022885974496603012\n",
            "Epoch 9, Batch 360, Loss: 0.027891136705875397\n",
            "Epoch 9, Batch 370, Loss: 0.02519177831709385\n",
            "Epoch 9, Batch 380, Loss: 0.018135491758584976\n",
            "Epoch 9, Batch 390, Loss: 0.02265028841793537\n",
            "Epoch 9, Batch 400, Loss: 0.016975175589323044\n",
            "Epoch 9, Batch 410, Loss: 0.019216367974877357\n",
            "Epoch 9, Batch 420, Loss: 0.016551479697227478\n",
            "Epoch 9, Batch 430, Loss: 0.019285477697849274\n",
            "Epoch 9, Batch 440, Loss: 0.016977455466985703\n",
            "Epoch 9, Batch 450, Loss: 0.01758715510368347\n",
            "Epoch 9, Batch 460, Loss: 0.023409562185406685\n",
            "Epoch 9, Batch 470, Loss: 0.01903114840388298\n",
            "Epoch 9, Batch 480, Loss: 0.015746120363473892\n",
            "Epoch 9, Batch 490, Loss: 0.016456257551908493\n",
            "Epoch 9, Batch 500, Loss: 0.02026628516614437\n",
            "Epoch 9, Batch 510, Loss: 0.024294696748256683\n",
            "Epoch 9, Batch 520, Loss: 0.015836097300052643\n",
            "Epoch 9, Batch 530, Loss: 0.02048363722860813\n",
            "Epoch 9, Batch 540, Loss: 0.019815718755126\n",
            "Epoch 9, Batch 550, Loss: 0.01262932550162077\n",
            "Epoch 9, Batch 560, Loss: 0.010938776656985283\n",
            "Epoch 9, Batch 570, Loss: 0.02123791165649891\n",
            "Epoch 9, Batch 580, Loss: 0.0207652784883976\n",
            "Epoch 9, Batch 590, Loss: 0.01749921403825283\n",
            "Epoch 9, Batch 600, Loss: 0.018882814794778824\n",
            "Epoch 9, Batch 610, Loss: 0.01623796671628952\n",
            "Epoch 9, Batch 620, Loss: 0.018623437732458115\n",
            "Epoch 9, Batch 630, Loss: 0.015526599250733852\n",
            "Epoch 9, Batch 640, Loss: 0.013454053550958633\n",
            "Epoch 9, Batch 650, Loss: 0.017697270959615707\n",
            "Epoch 9, Batch 660, Loss: 0.01562729850411415\n",
            "Epoch 9, Batch 670, Loss: 0.014073409140110016\n",
            "Epoch 9, Batch 680, Loss: 0.026130542159080505\n",
            "Epoch 9, Batch 690, Loss: 0.017101794481277466\n",
            "Epoch 9, Batch 700, Loss: 0.014572802931070328\n",
            "Epoch 9, Batch 710, Loss: 0.018224962055683136\n",
            "Epoch 9, Average Loss: 0.0002855172724869593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test dataset\n",
        "test_dataset = TensorDataset(test_user, test_course, test_grade)\n",
        "\n",
        "# Define test dataloader\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "oXhNmFr-7dS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test dataset\n",
        "model.eval()\n",
        "total_test_loss = 0.0\n",
        "num_test_samples = len(test_dataset)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        user, course, grade = batch\n",
        "        output = model(user, course)\n",
        "        loss = criterion(output, grade)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "test_mse = total_test_loss / num_test_samples\n",
        "print(f'Mean Squared Error (MSE) on the test dataset: {test_mse}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d87TZ94kQ4fS",
        "outputId": "2f264a83-462a-473c-e546-cf3204e6adc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) on the test dataset: 0.0002814433951916469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_courses(model, user_and_courses, top_k):\n",
        "    input_user, input_courses = user_and_courses\n",
        "\n",
        "    # Convert input_user to PyTorch tensor\n",
        "    user_ids = torch.LongTensor([input_user])\n",
        "\n",
        "    # Generate all possible course IDs\n",
        "    all_course_ids = torch.arange(len(le_course.classes_))\n",
        "\n",
        "    # Repeat the given user_id for all courses\n",
        "    user_ids = torch.full_like(all_course_ids, fill_value=user_ids[0])\n",
        "\n",
        "    # Make predictions for all courses for the given student\n",
        "    predictions = model(user_ids, all_course_ids).squeeze()\n",
        "\n",
        "    # Exclude courses already in input_courses from recommendations\n",
        "    for course, grade in input_courses.items():\n",
        "        course_index = le_course.transform([course])[0]\n",
        "        if course_index < len(predictions):\n",
        "            predictions[course_index] = float('-inf')\n",
        "\n",
        "    # Get the indices of the top-k predictions\n",
        "    num_recommendations = min(top_k, len(predictions))\n",
        "    top_indices = torch.topk(predictions, num_recommendations).indices\n",
        "\n",
        "    # Map the top indices back to the course IDs\n",
        "    top_course_ids = le_course.inverse_transform(top_indices.numpy())\n",
        "\n",
        "    # Exclude courses already in input_courses from recommendations (additional check)\n",
        "    top_course_ids = [course_id for course_id in top_course_ids if course_id not in input_courses]\n",
        "\n",
        "    return top_course_ids\n"
      ],
      "metadata": {
        "id": "vB-DTKASQ9-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Input student's grade for previous courses\n",
        "input_user = 123  # Replace with the actual student ID\n",
        "\n",
        "# Use the actual course labels seen during training\n",
        "input_courses = {\n",
        "    'CHEMISTRY LABORATORY': 8,\n",
        "    'GENERAL CHEMISTRY': 7,\n",
        "    'ELECTRICAL SCIENCES': 3,\n",
        "    'ADDITIVE MANUFACTURING': 1,\n",
        "    'PRACTICE SCHOOL I':10,\n",
        "    'PHYSICS LABORATORY':5\n",
        "\n",
        "    # Add more courses and grades as needed\n",
        "}\n",
        "\n",
        "# Set top_k to the desired number\n",
        "top_k = 5\n",
        "\n",
        "# Call the function with the updated course grades\n",
        "user_and_courses = (input_user, input_courses)\n",
        "recommended_courses = recommend_courses(model, user_and_courses, top_k)\n",
        "\n",
        "print(f\"Top {top_k} recommended courses for the student based on previous grades: {recommended_courses}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrvpeMGt8I4b",
        "outputId": "932c48e9-9ba0-41fc-8d55-6c1e0330c8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommended courses for the student based on previous grades: ['PRACTICE SCHOOL II', 'DESIGN PROJECT', 'BIOLOGY LABORATORY', 'STUDY PROJECT', 'THESIS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KlKBlkIx8KmN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}