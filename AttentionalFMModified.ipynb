{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN15fKbpXdKJiGY4N9su/zu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishnu0920/Recommender_System_Models/blob/main/AttentionalFMModified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdenyhNB8bcD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('grade_data.csv')\n",
        "\n",
        "# Encode student_id and course_id using LabelEncoder\n",
        "le_student = LabelEncoder()\n",
        "le_course = LabelEncoder()\n",
        "\n",
        "df['student_id'] = le_student.fit_transform(df['student_id'])\n",
        "df['course_id'] = le_course.fit_transform(df['course_id'])\n",
        "\n",
        "# Map course grades to the range [0, 1] for regression\n",
        "df['course_grade'] = df['course_grade'] / 10.0\n",
        "\n",
        "# Train-test split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "1F5q3Vxk8rkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data to PyTorch tensors\n",
        "train_user = torch.LongTensor(train_df['student_id'].values)\n",
        "train_course = torch.LongTensor(train_df['course_id'].values)\n",
        "train_grade = torch.FloatTensor(train_df['course_grade'].values)\n",
        "\n",
        "test_user = torch.LongTensor(test_df['student_id'].values)\n",
        "test_course = torch.LongTensor(test_df['course_id'].values)\n",
        "test_grade = torch.FloatTensor(test_df['course_grade'].values)\n"
      ],
      "metadata": {
        "id": "NjU_1atx8vBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionalFactorizationMachine(nn.Module):\n",
        "    def __init__(self, num_courses, embedding_dim):\n",
        "        super(AttentionalFactorizationMachine, self).__init__()\n",
        "\n",
        "        # Adjust the dimensions of embeddings and linear layer\n",
        "        self.course_embedding = nn.Embedding(num_courses, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, 1)\n",
        "        self.attention = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, course):\n",
        "        course_emb = self.course_embedding(course)\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = torch.sigmoid(self.attention(course_emb))\n",
        "        attention = attention_weights * course_emb\n",
        "\n",
        "        # Concatenate course embeddings with attention\n",
        "        course_attention = course_emb + attention\n",
        "\n",
        "        output = self.linear(course_attention)\n",
        "        return output.squeeze()\n"
      ],
      "metadata": {
        "id": "djISCZgc8vra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_users = len(le_student.classes_)\n",
        "num_courses = len(le_course.classes_)\n",
        "embedding_dim = 10\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_dataset = TensorDataset(train_user, train_course, train_grade)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Instantiate the model, define loss function and optimizer\n",
        "model = AttentionalFactorizationMachine(num_courses, embedding_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "FMDYCTq28xix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the print interval\n",
        "print_interval = 10  # Adjust the interval as needed\n",
        "\n",
        "# Cell 12: Training loop with performance monitoring\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch_index, batch in enumerate(train_dataloader):\n",
        "        _, course, grade = batch  # Remove user variable\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(course).squeeze()\n",
        "        loss = criterion(output, grade)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_index % print_interval == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UWkPmaE8zFy",
        "outputId": "960b7aa7-2acd-4d31-effc-1953a0d4f456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 0, Loss: 2.164921283721924\n",
            "Epoch 0, Batch 10, Loss: 1.6263986825942993\n",
            "Epoch 0, Batch 20, Loss: 2.104883909225464\n",
            "Epoch 0, Batch 30, Loss: 1.5321447849273682\n",
            "Epoch 0, Batch 40, Loss: 1.8873634338378906\n",
            "Epoch 0, Batch 50, Loss: 1.516208529472351\n",
            "Epoch 0, Batch 60, Loss: 1.7176568508148193\n",
            "Epoch 0, Batch 70, Loss: 1.2444063425064087\n",
            "Epoch 0, Batch 80, Loss: 1.1137255430221558\n",
            "Epoch 0, Batch 90, Loss: 0.9756931662559509\n",
            "Epoch 0, Batch 100, Loss: 1.1234780550003052\n",
            "Epoch 0, Batch 110, Loss: 1.0259339809417725\n",
            "Epoch 0, Batch 120, Loss: 1.0519884824752808\n",
            "Epoch 0, Batch 130, Loss: 0.9035086035728455\n",
            "Epoch 0, Batch 140, Loss: 0.9574010372161865\n",
            "Epoch 0, Batch 150, Loss: 0.8913798928260803\n",
            "Epoch 0, Batch 160, Loss: 0.8093476295471191\n",
            "Epoch 0, Batch 170, Loss: 0.8353613615036011\n",
            "Epoch 0, Batch 180, Loss: 0.7585614323616028\n",
            "Epoch 0, Batch 190, Loss: 0.757659912109375\n",
            "Epoch 0, Batch 200, Loss: 0.6633107662200928\n",
            "Epoch 0, Batch 210, Loss: 0.6984236836433411\n",
            "Epoch 0, Batch 220, Loss: 0.6843329071998596\n",
            "Epoch 0, Batch 230, Loss: 0.6535319089889526\n",
            "Epoch 0, Batch 240, Loss: 0.6049067378044128\n",
            "Epoch 0, Batch 250, Loss: 0.691353440284729\n",
            "Epoch 0, Batch 260, Loss: 0.6397346258163452\n",
            "Epoch 0, Batch 270, Loss: 0.5696343183517456\n",
            "Epoch 0, Batch 280, Loss: 0.49463337659835815\n",
            "Epoch 0, Batch 290, Loss: 0.578683078289032\n",
            "Epoch 0, Batch 300, Loss: 0.6716504096984863\n",
            "Epoch 0, Batch 310, Loss: 0.5406244397163391\n",
            "Epoch 0, Batch 320, Loss: 0.5020242929458618\n",
            "Epoch 0, Batch 330, Loss: 0.5124090909957886\n",
            "Epoch 0, Batch 340, Loss: 0.4100608229637146\n",
            "Epoch 0, Batch 350, Loss: 0.3938444256782532\n",
            "Epoch 0, Batch 360, Loss: 0.395553857088089\n",
            "Epoch 0, Batch 370, Loss: 0.3870764970779419\n",
            "Epoch 0, Batch 380, Loss: 0.4480690360069275\n",
            "Epoch 0, Batch 390, Loss: 0.3725551962852478\n",
            "Epoch 0, Batch 400, Loss: 0.38662445545196533\n",
            "Epoch 0, Batch 410, Loss: 0.36461859941482544\n",
            "Epoch 0, Batch 420, Loss: 0.44166299700737\n",
            "Epoch 0, Batch 430, Loss: 0.3862447142601013\n",
            "Epoch 0, Batch 440, Loss: 0.23527009785175323\n",
            "Epoch 0, Batch 450, Loss: 0.29770728945732117\n",
            "Epoch 0, Batch 460, Loss: 0.23169192671775818\n",
            "Epoch 0, Batch 470, Loss: 0.36486396193504333\n",
            "Epoch 0, Batch 480, Loss: 0.3359883725643158\n",
            "Epoch 0, Batch 490, Loss: 0.22210921347141266\n",
            "Epoch 0, Batch 500, Loss: 0.2734527885913849\n",
            "Epoch 0, Batch 510, Loss: 0.2271377444267273\n",
            "Epoch 0, Batch 520, Loss: 0.20746135711669922\n",
            "Epoch 0, Batch 530, Loss: 0.2193642556667328\n",
            "Epoch 0, Batch 540, Loss: 0.27639448642730713\n",
            "Epoch 0, Batch 550, Loss: 0.30569133162498474\n",
            "Epoch 0, Batch 560, Loss: 0.21921288967132568\n",
            "Epoch 0, Batch 570, Loss: 0.23336753249168396\n",
            "Epoch 0, Batch 580, Loss: 0.20445077121257782\n",
            "Epoch 0, Batch 590, Loss: 0.24975425004959106\n",
            "Epoch 0, Batch 600, Loss: 0.24668744206428528\n",
            "Epoch 0, Batch 610, Loss: 0.22040793299674988\n",
            "Epoch 0, Batch 620, Loss: 0.2012573629617691\n",
            "Epoch 0, Batch 630, Loss: 0.17528504133224487\n",
            "Epoch 0, Batch 640, Loss: 0.25122377276420593\n",
            "Epoch 0, Batch 650, Loss: 0.20265744626522064\n",
            "Epoch 0, Batch 660, Loss: 0.18634307384490967\n",
            "Epoch 0, Batch 670, Loss: 0.1574392169713974\n",
            "Epoch 0, Batch 680, Loss: 0.19920764863491058\n",
            "Epoch 0, Batch 690, Loss: 0.0997498631477356\n",
            "Epoch 0, Batch 700, Loss: 0.16606003046035767\n",
            "Epoch 0, Batch 710, Loss: 0.13451148569583893\n",
            "Epoch 1, Batch 0, Loss: 0.15882444381713867\n",
            "Epoch 1, Batch 10, Loss: 0.18053385615348816\n",
            "Epoch 1, Batch 20, Loss: 0.13564562797546387\n",
            "Epoch 1, Batch 30, Loss: 0.1417521834373474\n",
            "Epoch 1, Batch 40, Loss: 0.1312430500984192\n",
            "Epoch 1, Batch 50, Loss: 0.11931958794593811\n",
            "Epoch 1, Batch 60, Loss: 0.1461828649044037\n",
            "Epoch 1, Batch 70, Loss: 0.17520399391651154\n",
            "Epoch 1, Batch 80, Loss: 0.1505541205406189\n",
            "Epoch 1, Batch 90, Loss: 0.16862867772579193\n",
            "Epoch 1, Batch 100, Loss: 0.1200375035405159\n",
            "Epoch 1, Batch 110, Loss: 0.09800934791564941\n",
            "Epoch 1, Batch 120, Loss: 0.12343950569629669\n",
            "Epoch 1, Batch 130, Loss: 0.10540500283241272\n",
            "Epoch 1, Batch 140, Loss: 0.10103451460599899\n",
            "Epoch 1, Batch 150, Loss: 0.12047633528709412\n",
            "Epoch 1, Batch 160, Loss: 0.11424152553081512\n",
            "Epoch 1, Batch 170, Loss: 0.1306527554988861\n",
            "Epoch 1, Batch 180, Loss: 0.09092647582292557\n",
            "Epoch 1, Batch 190, Loss: 0.14649946987628937\n",
            "Epoch 1, Batch 200, Loss: 0.06141776591539383\n",
            "Epoch 1, Batch 210, Loss: 0.11328279972076416\n",
            "Epoch 1, Batch 220, Loss: 0.1256670504808426\n",
            "Epoch 1, Batch 230, Loss: 0.0818243995308876\n",
            "Epoch 1, Batch 240, Loss: 0.10687375068664551\n",
            "Epoch 1, Batch 250, Loss: 0.09437467902898788\n",
            "Epoch 1, Batch 260, Loss: 0.09879200905561447\n",
            "Epoch 1, Batch 270, Loss: 0.07514284551143646\n",
            "Epoch 1, Batch 280, Loss: 0.0712088942527771\n",
            "Epoch 1, Batch 290, Loss: 0.0726797953248024\n",
            "Epoch 1, Batch 300, Loss: 0.06821135431528091\n",
            "Epoch 1, Batch 310, Loss: 0.07846802473068237\n",
            "Epoch 1, Batch 320, Loss: 0.07561466842889786\n",
            "Epoch 1, Batch 330, Loss: 0.08494986593723297\n",
            "Epoch 1, Batch 340, Loss: 0.07456488907337189\n",
            "Epoch 1, Batch 350, Loss: 0.07804504036903381\n",
            "Epoch 1, Batch 360, Loss: 0.0814368799328804\n",
            "Epoch 1, Batch 370, Loss: 0.08773680776357651\n",
            "Epoch 1, Batch 380, Loss: 0.06670594960451126\n",
            "Epoch 1, Batch 390, Loss: 0.09772401303052902\n",
            "Epoch 1, Batch 400, Loss: 0.07214557379484177\n",
            "Epoch 1, Batch 410, Loss: 0.08089891821146011\n",
            "Epoch 1, Batch 420, Loss: 0.07677558064460754\n",
            "Epoch 1, Batch 430, Loss: 0.07253462076187134\n",
            "Epoch 1, Batch 440, Loss: 0.05824282020330429\n",
            "Epoch 1, Batch 450, Loss: 0.07225578278303146\n",
            "Epoch 1, Batch 460, Loss: 0.07860278338193893\n",
            "Epoch 1, Batch 470, Loss: 0.05893086642026901\n",
            "Epoch 1, Batch 480, Loss: 0.06044560298323631\n",
            "Epoch 1, Batch 490, Loss: 0.04817589744925499\n",
            "Epoch 1, Batch 500, Loss: 0.03357236087322235\n",
            "Epoch 1, Batch 510, Loss: 0.05785040184855461\n",
            "Epoch 1, Batch 520, Loss: 0.05093628168106079\n",
            "Epoch 1, Batch 530, Loss: 0.05463309586048126\n",
            "Epoch 1, Batch 540, Loss: 0.05531352385878563\n",
            "Epoch 1, Batch 550, Loss: 0.07291609048843384\n",
            "Epoch 1, Batch 560, Loss: 0.05986583232879639\n",
            "Epoch 1, Batch 570, Loss: 0.04573829472064972\n",
            "Epoch 1, Batch 580, Loss: 0.05238860845565796\n",
            "Epoch 1, Batch 590, Loss: 0.05731968954205513\n",
            "Epoch 1, Batch 600, Loss: 0.04068925231695175\n",
            "Epoch 1, Batch 610, Loss: 0.058095112442970276\n",
            "Epoch 1, Batch 620, Loss: 0.048360999673604965\n",
            "Epoch 1, Batch 630, Loss: 0.05301772058010101\n",
            "Epoch 1, Batch 640, Loss: 0.038949284702539444\n",
            "Epoch 1, Batch 650, Loss: 0.060809120535850525\n",
            "Epoch 1, Batch 660, Loss: 0.05032408982515335\n",
            "Epoch 1, Batch 670, Loss: 0.05546298623085022\n",
            "Epoch 1, Batch 680, Loss: 0.050750840455293655\n",
            "Epoch 1, Batch 690, Loss: 0.03655801713466644\n",
            "Epoch 1, Batch 700, Loss: 0.06119350343942642\n",
            "Epoch 1, Batch 710, Loss: 0.053667061030864716\n",
            "Epoch 2, Batch 0, Loss: 0.04003443941473961\n",
            "Epoch 2, Batch 10, Loss: 0.03421011567115784\n",
            "Epoch 2, Batch 20, Loss: 0.05210947245359421\n",
            "Epoch 2, Batch 30, Loss: 0.055445920675992966\n",
            "Epoch 2, Batch 40, Loss: 0.03652196004986763\n",
            "Epoch 2, Batch 50, Loss: 0.056563958525657654\n",
            "Epoch 2, Batch 60, Loss: 0.04084794595837593\n",
            "Epoch 2, Batch 70, Loss: 0.047126926481723785\n",
            "Epoch 2, Batch 80, Loss: 0.04970131069421768\n",
            "Epoch 2, Batch 90, Loss: 0.03894903138279915\n",
            "Epoch 2, Batch 100, Loss: 0.05639847368001938\n",
            "Epoch 2, Batch 110, Loss: 0.05313468351960182\n",
            "Epoch 2, Batch 120, Loss: 0.03678744286298752\n",
            "Epoch 2, Batch 130, Loss: 0.02215670794248581\n",
            "Epoch 2, Batch 140, Loss: 0.025974879041314125\n",
            "Epoch 2, Batch 150, Loss: 0.03517041355371475\n",
            "Epoch 2, Batch 160, Loss: 0.040225815027952194\n",
            "Epoch 2, Batch 170, Loss: 0.04315068945288658\n",
            "Epoch 2, Batch 180, Loss: 0.029684310778975487\n",
            "Epoch 2, Batch 190, Loss: 0.0349845290184021\n",
            "Epoch 2, Batch 200, Loss: 0.0293894000351429\n",
            "Epoch 2, Batch 210, Loss: 0.04396872594952583\n",
            "Epoch 2, Batch 220, Loss: 0.03354635834693909\n",
            "Epoch 2, Batch 230, Loss: 0.04807978868484497\n",
            "Epoch 2, Batch 240, Loss: 0.028475066646933556\n",
            "Epoch 2, Batch 250, Loss: 0.027381736785173416\n",
            "Epoch 2, Batch 260, Loss: 0.03662857413291931\n",
            "Epoch 2, Batch 270, Loss: 0.04082660377025604\n",
            "Epoch 2, Batch 280, Loss: 0.03642696514725685\n",
            "Epoch 2, Batch 290, Loss: 0.03731483221054077\n",
            "Epoch 2, Batch 300, Loss: 0.035883828997612\n",
            "Epoch 2, Batch 310, Loss: 0.021206587553024292\n",
            "Epoch 2, Batch 320, Loss: 0.04053749889135361\n",
            "Epoch 2, Batch 330, Loss: 0.03473709151148796\n",
            "Epoch 2, Batch 340, Loss: 0.02366206981241703\n",
            "Epoch 2, Batch 350, Loss: 0.03494182601571083\n",
            "Epoch 2, Batch 360, Loss: 0.038596972823143005\n",
            "Epoch 2, Batch 370, Loss: 0.03240001201629639\n",
            "Epoch 2, Batch 380, Loss: 0.02842659503221512\n",
            "Epoch 2, Batch 390, Loss: 0.036733195185661316\n",
            "Epoch 2, Batch 400, Loss: 0.03560173138976097\n",
            "Epoch 2, Batch 410, Loss: 0.028747253119945526\n",
            "Epoch 2, Batch 420, Loss: 0.030434664338827133\n",
            "Epoch 2, Batch 430, Loss: 0.045805301517248154\n",
            "Epoch 2, Batch 440, Loss: 0.01961163990199566\n",
            "Epoch 2, Batch 450, Loss: 0.04105716198682785\n",
            "Epoch 2, Batch 460, Loss: 0.025198567658662796\n",
            "Epoch 2, Batch 470, Loss: 0.035488635301589966\n",
            "Epoch 2, Batch 480, Loss: 0.03526140749454498\n",
            "Epoch 2, Batch 490, Loss: 0.035443276166915894\n",
            "Epoch 2, Batch 500, Loss: 0.026597455143928528\n",
            "Epoch 2, Batch 510, Loss: 0.02782382071018219\n",
            "Epoch 2, Batch 520, Loss: 0.04389156028628349\n",
            "Epoch 2, Batch 530, Loss: 0.037720900028944016\n",
            "Epoch 2, Batch 540, Loss: 0.022665422409772873\n",
            "Epoch 2, Batch 550, Loss: 0.026746954768896103\n",
            "Epoch 2, Batch 560, Loss: 0.025781475007534027\n",
            "Epoch 2, Batch 570, Loss: 0.019691042602062225\n",
            "Epoch 2, Batch 580, Loss: 0.04122937470674515\n",
            "Epoch 2, Batch 590, Loss: 0.022075412794947624\n",
            "Epoch 2, Batch 600, Loss: 0.02214537002146244\n",
            "Epoch 2, Batch 610, Loss: 0.043299660086631775\n",
            "Epoch 2, Batch 620, Loss: 0.038976170122623444\n",
            "Epoch 2, Batch 630, Loss: 0.030593393370509148\n",
            "Epoch 2, Batch 640, Loss: 0.031598806381225586\n",
            "Epoch 2, Batch 650, Loss: 0.03297281637787819\n",
            "Epoch 2, Batch 660, Loss: 0.03249828517436981\n",
            "Epoch 2, Batch 670, Loss: 0.028902681544423103\n",
            "Epoch 2, Batch 680, Loss: 0.028756987303495407\n",
            "Epoch 2, Batch 690, Loss: 0.03339375555515289\n",
            "Epoch 2, Batch 700, Loss: 0.023127641528844833\n",
            "Epoch 2, Batch 710, Loss: 0.026119699701666832\n",
            "Epoch 3, Batch 0, Loss: 0.02498118206858635\n",
            "Epoch 3, Batch 10, Loss: 0.035977333784103394\n",
            "Epoch 3, Batch 20, Loss: 0.035727888345718384\n",
            "Epoch 3, Batch 30, Loss: 0.02130413055419922\n",
            "Epoch 3, Batch 40, Loss: 0.021283093839883804\n",
            "Epoch 3, Batch 50, Loss: 0.030020151287317276\n",
            "Epoch 3, Batch 60, Loss: 0.025989724323153496\n",
            "Epoch 3, Batch 70, Loss: 0.0238712839782238\n",
            "Epoch 3, Batch 80, Loss: 0.03359493613243103\n",
            "Epoch 3, Batch 90, Loss: 0.020625639706850052\n",
            "Epoch 3, Batch 100, Loss: 0.02816024236381054\n",
            "Epoch 3, Batch 110, Loss: 0.030227193608880043\n",
            "Epoch 3, Batch 120, Loss: 0.02500341460108757\n",
            "Epoch 3, Batch 130, Loss: 0.024677418172359467\n",
            "Epoch 3, Batch 140, Loss: 0.03806918114423752\n",
            "Epoch 3, Batch 150, Loss: 0.024311361834406853\n",
            "Epoch 3, Batch 160, Loss: 0.03474948927760124\n",
            "Epoch 3, Batch 170, Loss: 0.02612397074699402\n",
            "Epoch 3, Batch 180, Loss: 0.033321186900138855\n",
            "Epoch 3, Batch 190, Loss: 0.019526006653904915\n",
            "Epoch 3, Batch 200, Loss: 0.03031528927385807\n",
            "Epoch 3, Batch 210, Loss: 0.0269255843013525\n",
            "Epoch 3, Batch 220, Loss: 0.031126080080866814\n",
            "Epoch 3, Batch 230, Loss: 0.02708197385072708\n",
            "Epoch 3, Batch 240, Loss: 0.014242947101593018\n",
            "Epoch 3, Batch 250, Loss: 0.03348410502076149\n",
            "Epoch 3, Batch 260, Loss: 0.025056421756744385\n",
            "Epoch 3, Batch 270, Loss: 0.020733168348670006\n",
            "Epoch 3, Batch 280, Loss: 0.028042295947670937\n",
            "Epoch 3, Batch 290, Loss: 0.03100057877600193\n",
            "Epoch 3, Batch 300, Loss: 0.036604419350624084\n",
            "Epoch 3, Batch 310, Loss: 0.016963332891464233\n",
            "Epoch 3, Batch 320, Loss: 0.024605251848697662\n",
            "Epoch 3, Batch 330, Loss: 0.038858383893966675\n",
            "Epoch 3, Batch 340, Loss: 0.0278268214315176\n",
            "Epoch 3, Batch 350, Loss: 0.012540729716420174\n",
            "Epoch 3, Batch 360, Loss: 0.02775568515062332\n",
            "Epoch 3, Batch 370, Loss: 0.0238143689930439\n",
            "Epoch 3, Batch 380, Loss: 0.01949637196958065\n",
            "Epoch 3, Batch 390, Loss: 0.020844969898462296\n",
            "Epoch 3, Batch 400, Loss: 0.020941680297255516\n",
            "Epoch 3, Batch 410, Loss: 0.026762308552861214\n",
            "Epoch 3, Batch 420, Loss: 0.020824164152145386\n",
            "Epoch 3, Batch 430, Loss: 0.02568206936120987\n",
            "Epoch 3, Batch 440, Loss: 0.025295821949839592\n",
            "Epoch 3, Batch 450, Loss: 0.035872526466846466\n",
            "Epoch 3, Batch 460, Loss: 0.02256188727915287\n",
            "Epoch 3, Batch 470, Loss: 0.01621266081929207\n",
            "Epoch 3, Batch 480, Loss: 0.02441227249801159\n",
            "Epoch 3, Batch 490, Loss: 0.022139493376016617\n",
            "Epoch 3, Batch 500, Loss: 0.02667980268597603\n",
            "Epoch 3, Batch 510, Loss: 0.02558209002017975\n",
            "Epoch 3, Batch 520, Loss: 0.021905776113271713\n",
            "Epoch 3, Batch 530, Loss: 0.02102820575237274\n",
            "Epoch 3, Batch 540, Loss: 0.030363567173480988\n",
            "Epoch 3, Batch 550, Loss: 0.026305504143238068\n",
            "Epoch 3, Batch 560, Loss: 0.019505860283970833\n",
            "Epoch 3, Batch 570, Loss: 0.02889816276729107\n",
            "Epoch 3, Batch 580, Loss: 0.03319832310080528\n",
            "Epoch 3, Batch 590, Loss: 0.024010706692934036\n",
            "Epoch 3, Batch 600, Loss: 0.02909577079117298\n",
            "Epoch 3, Batch 610, Loss: 0.026223063468933105\n",
            "Epoch 3, Batch 620, Loss: 0.020755788311362267\n",
            "Epoch 3, Batch 630, Loss: 0.022547997534275055\n",
            "Epoch 3, Batch 640, Loss: 0.014733418822288513\n",
            "Epoch 3, Batch 650, Loss: 0.021550253033638\n",
            "Epoch 3, Batch 660, Loss: 0.025109488517045975\n",
            "Epoch 3, Batch 670, Loss: 0.027277963235974312\n",
            "Epoch 3, Batch 680, Loss: 0.02332725189626217\n",
            "Epoch 3, Batch 690, Loss: 0.02449779585003853\n",
            "Epoch 3, Batch 700, Loss: 0.02346503734588623\n",
            "Epoch 3, Batch 710, Loss: 0.027664829045534134\n",
            "Epoch 4, Batch 0, Loss: 0.020768309012055397\n",
            "Epoch 4, Batch 10, Loss: 0.030149511992931366\n",
            "Epoch 4, Batch 20, Loss: 0.032697830349206924\n",
            "Epoch 4, Batch 30, Loss: 0.024950852617621422\n",
            "Epoch 4, Batch 40, Loss: 0.028135379776358604\n",
            "Epoch 4, Batch 50, Loss: 0.025223467499017715\n",
            "Epoch 4, Batch 60, Loss: 0.01932373084127903\n",
            "Epoch 4, Batch 70, Loss: 0.027428308501839638\n",
            "Epoch 4, Batch 80, Loss: 0.02859479933977127\n",
            "Epoch 4, Batch 90, Loss: 0.023788249120116234\n",
            "Epoch 4, Batch 100, Loss: 0.01926891878247261\n",
            "Epoch 4, Batch 110, Loss: 0.017664361745119095\n",
            "Epoch 4, Batch 120, Loss: 0.021638579666614532\n",
            "Epoch 4, Batch 130, Loss: 0.01523912139236927\n",
            "Epoch 4, Batch 140, Loss: 0.0323885940015316\n",
            "Epoch 4, Batch 150, Loss: 0.032281722873449326\n",
            "Epoch 4, Batch 160, Loss: 0.030528616160154343\n",
            "Epoch 4, Batch 170, Loss: 0.02617543190717697\n",
            "Epoch 4, Batch 180, Loss: 0.03141879662871361\n",
            "Epoch 4, Batch 190, Loss: 0.02535409852862358\n",
            "Epoch 4, Batch 200, Loss: 0.022588413208723068\n",
            "Epoch 4, Batch 210, Loss: 0.02485792338848114\n",
            "Epoch 4, Batch 220, Loss: 0.023122675716876984\n",
            "Epoch 4, Batch 230, Loss: 0.017183024436235428\n",
            "Epoch 4, Batch 240, Loss: 0.01717313379049301\n",
            "Epoch 4, Batch 250, Loss: 0.03386429697275162\n",
            "Epoch 4, Batch 260, Loss: 0.026296410709619522\n",
            "Epoch 4, Batch 270, Loss: 0.020710214972496033\n",
            "Epoch 4, Batch 280, Loss: 0.03333228826522827\n",
            "Epoch 4, Batch 290, Loss: 0.027650687843561172\n",
            "Epoch 4, Batch 300, Loss: 0.021340101957321167\n",
            "Epoch 4, Batch 310, Loss: 0.03411310166120529\n",
            "Epoch 4, Batch 320, Loss: 0.02482735738158226\n",
            "Epoch 4, Batch 330, Loss: 0.018290000036358833\n",
            "Epoch 4, Batch 340, Loss: 0.028399579226970673\n",
            "Epoch 4, Batch 350, Loss: 0.020058762282133102\n",
            "Epoch 4, Batch 360, Loss: 0.019387241452932358\n",
            "Epoch 4, Batch 370, Loss: 0.020848438143730164\n",
            "Epoch 4, Batch 380, Loss: 0.02122129499912262\n",
            "Epoch 4, Batch 390, Loss: 0.03748003765940666\n",
            "Epoch 4, Batch 400, Loss: 0.026505624875426292\n",
            "Epoch 4, Batch 410, Loss: 0.0210968479514122\n",
            "Epoch 4, Batch 420, Loss: 0.02213868498802185\n",
            "Epoch 4, Batch 430, Loss: 0.02445434406399727\n",
            "Epoch 4, Batch 440, Loss: 0.014947419054806232\n",
            "Epoch 4, Batch 450, Loss: 0.03033383935689926\n",
            "Epoch 4, Batch 460, Loss: 0.02230898290872574\n",
            "Epoch 4, Batch 470, Loss: 0.03156167268753052\n",
            "Epoch 4, Batch 480, Loss: 0.03000912070274353\n",
            "Epoch 4, Batch 490, Loss: 0.0232289619743824\n",
            "Epoch 4, Batch 500, Loss: 0.019254043698310852\n",
            "Epoch 4, Batch 510, Loss: 0.02231096662580967\n",
            "Epoch 4, Batch 520, Loss: 0.021381041035056114\n",
            "Epoch 4, Batch 530, Loss: 0.017169572412967682\n",
            "Epoch 4, Batch 540, Loss: 0.022852636873722076\n",
            "Epoch 4, Batch 550, Loss: 0.0189580786973238\n",
            "Epoch 4, Batch 560, Loss: 0.021225370466709137\n",
            "Epoch 4, Batch 570, Loss: 0.02688269317150116\n",
            "Epoch 4, Batch 580, Loss: 0.027759240940213203\n",
            "Epoch 4, Batch 590, Loss: 0.01833883486688137\n",
            "Epoch 4, Batch 600, Loss: 0.027280595153570175\n",
            "Epoch 4, Batch 610, Loss: 0.015463951975107193\n",
            "Epoch 4, Batch 620, Loss: 0.02233370952308178\n",
            "Epoch 4, Batch 630, Loss: 0.017589647322893143\n",
            "Epoch 4, Batch 640, Loss: 0.02565600909292698\n",
            "Epoch 4, Batch 650, Loss: 0.015786588191986084\n",
            "Epoch 4, Batch 660, Loss: 0.021754000335931778\n",
            "Epoch 4, Batch 670, Loss: 0.028772661462426186\n",
            "Epoch 4, Batch 680, Loss: 0.023876909166574478\n",
            "Epoch 4, Batch 690, Loss: 0.02510131150484085\n",
            "Epoch 4, Batch 700, Loss: 0.018539147451519966\n",
            "Epoch 4, Batch 710, Loss: 0.03538517653942108\n",
            "Epoch 5, Batch 0, Loss: 0.030508169904351234\n",
            "Epoch 5, Batch 10, Loss: 0.018063925206661224\n",
            "Epoch 5, Batch 20, Loss: 0.01862677000463009\n",
            "Epoch 5, Batch 30, Loss: 0.02790934592485428\n",
            "Epoch 5, Batch 40, Loss: 0.02350035309791565\n",
            "Epoch 5, Batch 50, Loss: 0.019693080335855484\n",
            "Epoch 5, Batch 60, Loss: 0.020608920603990555\n",
            "Epoch 5, Batch 70, Loss: 0.020990479737520218\n",
            "Epoch 5, Batch 80, Loss: 0.018490660935640335\n",
            "Epoch 5, Batch 90, Loss: 0.022520966827869415\n",
            "Epoch 5, Batch 100, Loss: 0.026064444333314896\n",
            "Epoch 5, Batch 110, Loss: 0.02821981906890869\n",
            "Epoch 5, Batch 120, Loss: 0.02788349986076355\n",
            "Epoch 5, Batch 130, Loss: 0.016086630523204803\n",
            "Epoch 5, Batch 140, Loss: 0.017915645614266396\n",
            "Epoch 5, Batch 150, Loss: 0.02854073792695999\n",
            "Epoch 5, Batch 160, Loss: 0.024425510317087173\n",
            "Epoch 5, Batch 170, Loss: 0.021324632689356804\n",
            "Epoch 5, Batch 180, Loss: 0.025469789281487465\n",
            "Epoch 5, Batch 190, Loss: 0.029021624475717545\n",
            "Epoch 5, Batch 200, Loss: 0.019158009439706802\n",
            "Epoch 5, Batch 210, Loss: 0.02524521015584469\n",
            "Epoch 5, Batch 220, Loss: 0.01991640217602253\n",
            "Epoch 5, Batch 230, Loss: 0.02837151475250721\n",
            "Epoch 5, Batch 240, Loss: 0.020052973181009293\n",
            "Epoch 5, Batch 250, Loss: 0.02417716197669506\n",
            "Epoch 5, Batch 260, Loss: 0.02385939471423626\n",
            "Epoch 5, Batch 270, Loss: 0.01779128424823284\n",
            "Epoch 5, Batch 280, Loss: 0.028918303549289703\n",
            "Epoch 5, Batch 290, Loss: 0.026031939312815666\n",
            "Epoch 5, Batch 300, Loss: 0.020329266786575317\n",
            "Epoch 5, Batch 310, Loss: 0.02504918724298477\n",
            "Epoch 5, Batch 320, Loss: 0.020151657983660698\n",
            "Epoch 5, Batch 330, Loss: 0.02979050576686859\n",
            "Epoch 5, Batch 340, Loss: 0.027473684400320053\n",
            "Epoch 5, Batch 350, Loss: 0.017784159630537033\n",
            "Epoch 5, Batch 360, Loss: 0.0314321294426918\n",
            "Epoch 5, Batch 370, Loss: 0.012262723408639431\n",
            "Epoch 5, Batch 380, Loss: 0.02428528666496277\n",
            "Epoch 5, Batch 390, Loss: 0.01998279057443142\n",
            "Epoch 5, Batch 400, Loss: 0.027438078075647354\n",
            "Epoch 5, Batch 410, Loss: 0.022574346512556076\n",
            "Epoch 5, Batch 420, Loss: 0.026471775025129318\n",
            "Epoch 5, Batch 430, Loss: 0.019650856032967567\n",
            "Epoch 5, Batch 440, Loss: 0.02113230712711811\n",
            "Epoch 5, Batch 450, Loss: 0.029240388423204422\n",
            "Epoch 5, Batch 460, Loss: 0.020075608044862747\n",
            "Epoch 5, Batch 470, Loss: 0.022026503458619118\n",
            "Epoch 5, Batch 480, Loss: 0.028145376592874527\n",
            "Epoch 5, Batch 490, Loss: 0.025246653705835342\n",
            "Epoch 5, Batch 500, Loss: 0.018158962950110435\n",
            "Epoch 5, Batch 510, Loss: 0.01728302612900734\n",
            "Epoch 5, Batch 520, Loss: 0.019076382741332054\n",
            "Epoch 5, Batch 530, Loss: 0.015274365432560444\n",
            "Epoch 5, Batch 540, Loss: 0.0247671976685524\n",
            "Epoch 5, Batch 550, Loss: 0.02019323781132698\n",
            "Epoch 5, Batch 560, Loss: 0.03209734708070755\n",
            "Epoch 5, Batch 570, Loss: 0.019524721428751945\n",
            "Epoch 5, Batch 580, Loss: 0.021001961082220078\n",
            "Epoch 5, Batch 590, Loss: 0.02075946517288685\n",
            "Epoch 5, Batch 600, Loss: 0.018768658861517906\n",
            "Epoch 5, Batch 610, Loss: 0.024968573823571205\n",
            "Epoch 5, Batch 620, Loss: 0.03421497344970703\n",
            "Epoch 5, Batch 630, Loss: 0.020483484491705894\n",
            "Epoch 5, Batch 640, Loss: 0.021616892889142036\n",
            "Epoch 5, Batch 650, Loss: 0.024900591000914574\n",
            "Epoch 5, Batch 660, Loss: 0.018551308661699295\n",
            "Epoch 5, Batch 670, Loss: 0.02043791115283966\n",
            "Epoch 5, Batch 680, Loss: 0.02535528689622879\n",
            "Epoch 5, Batch 690, Loss: 0.027549520134925842\n",
            "Epoch 5, Batch 700, Loss: 0.020143939182162285\n",
            "Epoch 5, Batch 710, Loss: 0.024070223793387413\n",
            "Epoch 6, Batch 0, Loss: 0.016994992271065712\n",
            "Epoch 6, Batch 10, Loss: 0.03086109459400177\n",
            "Epoch 6, Batch 20, Loss: 0.023317601531744003\n",
            "Epoch 6, Batch 30, Loss: 0.01716555282473564\n",
            "Epoch 6, Batch 40, Loss: 0.02891652286052704\n",
            "Epoch 6, Batch 50, Loss: 0.01453669648617506\n",
            "Epoch 6, Batch 60, Loss: 0.016219496726989746\n",
            "Epoch 6, Batch 70, Loss: 0.02499024197459221\n",
            "Epoch 6, Batch 80, Loss: 0.021408366039395332\n",
            "Epoch 6, Batch 90, Loss: 0.020565882325172424\n",
            "Epoch 6, Batch 100, Loss: 0.016568968072533607\n",
            "Epoch 6, Batch 110, Loss: 0.03692057356238365\n",
            "Epoch 6, Batch 120, Loss: 0.02179468609392643\n",
            "Epoch 6, Batch 130, Loss: 0.02453911490738392\n",
            "Epoch 6, Batch 140, Loss: 0.018388984724879265\n",
            "Epoch 6, Batch 150, Loss: 0.030626069754362106\n",
            "Epoch 6, Batch 160, Loss: 0.02151058241724968\n",
            "Epoch 6, Batch 170, Loss: 0.021516045555472374\n",
            "Epoch 6, Batch 180, Loss: 0.01810644008219242\n",
            "Epoch 6, Batch 190, Loss: 0.022620556876063347\n",
            "Epoch 6, Batch 200, Loss: 0.030893446877598763\n",
            "Epoch 6, Batch 210, Loss: 0.029337281361222267\n",
            "Epoch 6, Batch 220, Loss: 0.016988271847367287\n",
            "Epoch 6, Batch 230, Loss: 0.018767962232232094\n",
            "Epoch 6, Batch 240, Loss: 0.032861679792404175\n",
            "Epoch 6, Batch 250, Loss: 0.0213913694024086\n",
            "Epoch 6, Batch 260, Loss: 0.022442080080509186\n",
            "Epoch 6, Batch 270, Loss: 0.021211756393313408\n",
            "Epoch 6, Batch 280, Loss: 0.030121127143502235\n",
            "Epoch 6, Batch 290, Loss: 0.025026915594935417\n",
            "Epoch 6, Batch 300, Loss: 0.02319576032459736\n",
            "Epoch 6, Batch 310, Loss: 0.02116151712834835\n",
            "Epoch 6, Batch 320, Loss: 0.0280016977339983\n",
            "Epoch 6, Batch 330, Loss: 0.02523062564432621\n",
            "Epoch 6, Batch 340, Loss: 0.01879362389445305\n",
            "Epoch 6, Batch 350, Loss: 0.02889743074774742\n",
            "Epoch 6, Batch 360, Loss: 0.0332830585539341\n",
            "Epoch 6, Batch 370, Loss: 0.022445647045969963\n",
            "Epoch 6, Batch 380, Loss: 0.02477480284869671\n",
            "Epoch 6, Batch 390, Loss: 0.02105683833360672\n",
            "Epoch 6, Batch 400, Loss: 0.026419438421726227\n",
            "Epoch 6, Batch 410, Loss: 0.022070452570915222\n",
            "Epoch 6, Batch 420, Loss: 0.025203224271535873\n",
            "Epoch 6, Batch 430, Loss: 0.02078157477080822\n",
            "Epoch 6, Batch 440, Loss: 0.019482113420963287\n",
            "Epoch 6, Batch 450, Loss: 0.024865461513400078\n",
            "Epoch 6, Batch 460, Loss: 0.018325556069612503\n",
            "Epoch 6, Batch 470, Loss: 0.03719674050807953\n",
            "Epoch 6, Batch 480, Loss: 0.028956549242138863\n",
            "Epoch 6, Batch 490, Loss: 0.024046363309025764\n",
            "Epoch 6, Batch 500, Loss: 0.01805594190955162\n",
            "Epoch 6, Batch 510, Loss: 0.026663288474082947\n",
            "Epoch 6, Batch 520, Loss: 0.020965654402971268\n",
            "Epoch 6, Batch 530, Loss: 0.02126026526093483\n",
            "Epoch 6, Batch 540, Loss: 0.021905170753598213\n",
            "Epoch 6, Batch 550, Loss: 0.023549426347017288\n",
            "Epoch 6, Batch 560, Loss: 0.024697061628103256\n",
            "Epoch 6, Batch 570, Loss: 0.01659185253083706\n",
            "Epoch 6, Batch 580, Loss: 0.02762654423713684\n",
            "Epoch 6, Batch 590, Loss: 0.01407112181186676\n",
            "Epoch 6, Batch 600, Loss: 0.023771194741129875\n",
            "Epoch 6, Batch 610, Loss: 0.018217969685792923\n",
            "Epoch 6, Batch 620, Loss: 0.025558900088071823\n",
            "Epoch 6, Batch 630, Loss: 0.013949903659522533\n",
            "Epoch 6, Batch 640, Loss: 0.026263630017638206\n",
            "Epoch 6, Batch 650, Loss: 0.024680227041244507\n",
            "Epoch 6, Batch 660, Loss: 0.02947346493601799\n",
            "Epoch 6, Batch 670, Loss: 0.020924217998981476\n",
            "Epoch 6, Batch 680, Loss: 0.023156045004725456\n",
            "Epoch 6, Batch 690, Loss: 0.024705467745661736\n",
            "Epoch 6, Batch 700, Loss: 0.02205052226781845\n",
            "Epoch 6, Batch 710, Loss: 0.025659292936325073\n",
            "Epoch 7, Batch 0, Loss: 0.01942310854792595\n",
            "Epoch 7, Batch 10, Loss: 0.020613299682736397\n",
            "Epoch 7, Batch 20, Loss: 0.02591191977262497\n",
            "Epoch 7, Batch 30, Loss: 0.023962369188666344\n",
            "Epoch 7, Batch 40, Loss: 0.016635950654745102\n",
            "Epoch 7, Batch 50, Loss: 0.01757621392607689\n",
            "Epoch 7, Batch 60, Loss: 0.019557349383831024\n",
            "Epoch 7, Batch 70, Loss: 0.02222532220184803\n",
            "Epoch 7, Batch 80, Loss: 0.024255836382508278\n",
            "Epoch 7, Batch 90, Loss: 0.022505205124616623\n",
            "Epoch 7, Batch 100, Loss: 0.02234721928834915\n",
            "Epoch 7, Batch 110, Loss: 0.02061193250119686\n",
            "Epoch 7, Batch 120, Loss: 0.0178787000477314\n",
            "Epoch 7, Batch 130, Loss: 0.025965970009565353\n",
            "Epoch 7, Batch 140, Loss: 0.023860124871134758\n",
            "Epoch 7, Batch 150, Loss: 0.027726763859391212\n",
            "Epoch 7, Batch 160, Loss: 0.022281454876065254\n",
            "Epoch 7, Batch 170, Loss: 0.02655881457030773\n",
            "Epoch 7, Batch 180, Loss: 0.02534875087440014\n",
            "Epoch 7, Batch 190, Loss: 0.02511760964989662\n",
            "Epoch 7, Batch 200, Loss: 0.02417665347456932\n",
            "Epoch 7, Batch 210, Loss: 0.020258676260709763\n",
            "Epoch 7, Batch 220, Loss: 0.026823582127690315\n",
            "Epoch 7, Batch 230, Loss: 0.018078984692692757\n",
            "Epoch 7, Batch 240, Loss: 0.020729543641209602\n",
            "Epoch 7, Batch 250, Loss: 0.018813351169228554\n",
            "Epoch 7, Batch 260, Loss: 0.029456868767738342\n",
            "Epoch 7, Batch 270, Loss: 0.01971355825662613\n",
            "Epoch 7, Batch 280, Loss: 0.017119625583291054\n",
            "Epoch 7, Batch 290, Loss: 0.015284081920981407\n",
            "Epoch 7, Batch 300, Loss: 0.01916325092315674\n",
            "Epoch 7, Batch 310, Loss: 0.018744027242064476\n",
            "Epoch 7, Batch 320, Loss: 0.022201959043741226\n",
            "Epoch 7, Batch 330, Loss: 0.01726328767836094\n",
            "Epoch 7, Batch 340, Loss: 0.022544823586940765\n",
            "Epoch 7, Batch 350, Loss: 0.022417090833187103\n",
            "Epoch 7, Batch 360, Loss: 0.03665908798575401\n",
            "Epoch 7, Batch 370, Loss: 0.017407411709427834\n",
            "Epoch 7, Batch 380, Loss: 0.01823657564818859\n",
            "Epoch 7, Batch 390, Loss: 0.02003570832312107\n",
            "Epoch 7, Batch 400, Loss: 0.01966891437768936\n",
            "Epoch 7, Batch 410, Loss: 0.01693171262741089\n",
            "Epoch 7, Batch 420, Loss: 0.02259502187371254\n",
            "Epoch 7, Batch 430, Loss: 0.025005390867590904\n",
            "Epoch 7, Batch 440, Loss: 0.01891779713332653\n",
            "Epoch 7, Batch 450, Loss: 0.022193197160959244\n",
            "Epoch 7, Batch 460, Loss: 0.02366620860993862\n",
            "Epoch 7, Batch 470, Loss: 0.02502945065498352\n",
            "Epoch 7, Batch 480, Loss: 0.01910550706088543\n",
            "Epoch 7, Batch 490, Loss: 0.030164478346705437\n",
            "Epoch 7, Batch 500, Loss: 0.02092350646853447\n",
            "Epoch 7, Batch 510, Loss: 0.01820417121052742\n",
            "Epoch 7, Batch 520, Loss: 0.013351678848266602\n",
            "Epoch 7, Batch 530, Loss: 0.018682332709431648\n",
            "Epoch 7, Batch 540, Loss: 0.019367167726159096\n",
            "Epoch 7, Batch 550, Loss: 0.02026735618710518\n",
            "Epoch 7, Batch 560, Loss: 0.02091515250504017\n",
            "Epoch 7, Batch 570, Loss: 0.023710133507847786\n",
            "Epoch 7, Batch 580, Loss: 0.01971897855401039\n",
            "Epoch 7, Batch 590, Loss: 0.021095240488648415\n",
            "Epoch 7, Batch 600, Loss: 0.021766994148492813\n",
            "Epoch 7, Batch 610, Loss: 0.0292675644159317\n",
            "Epoch 7, Batch 620, Loss: 0.025920139625668526\n",
            "Epoch 7, Batch 630, Loss: 0.02242244780063629\n",
            "Epoch 7, Batch 640, Loss: 0.018476396799087524\n",
            "Epoch 7, Batch 650, Loss: 0.021728763356804848\n",
            "Epoch 7, Batch 660, Loss: 0.018456673249602318\n",
            "Epoch 7, Batch 670, Loss: 0.0180791225284338\n",
            "Epoch 7, Batch 680, Loss: 0.015215840190649033\n",
            "Epoch 7, Batch 690, Loss: 0.020588628947734833\n",
            "Epoch 7, Batch 700, Loss: 0.021681345999240875\n",
            "Epoch 7, Batch 710, Loss: 0.023991316556930542\n",
            "Epoch 8, Batch 0, Loss: 0.01915387623012066\n",
            "Epoch 8, Batch 10, Loss: 0.014849075116217136\n",
            "Epoch 8, Batch 20, Loss: 0.020572740584611893\n",
            "Epoch 8, Batch 30, Loss: 0.018541652709245682\n",
            "Epoch 8, Batch 40, Loss: 0.029847854748368263\n",
            "Epoch 8, Batch 50, Loss: 0.032260142266750336\n",
            "Epoch 8, Batch 60, Loss: 0.015577446669340134\n",
            "Epoch 8, Batch 70, Loss: 0.025048691779375076\n",
            "Epoch 8, Batch 80, Loss: 0.01680120825767517\n",
            "Epoch 8, Batch 90, Loss: 0.014806119725108147\n",
            "Epoch 8, Batch 100, Loss: 0.019577503204345703\n",
            "Epoch 8, Batch 110, Loss: 0.024791300296783447\n",
            "Epoch 8, Batch 120, Loss: 0.020324239507317543\n",
            "Epoch 8, Batch 130, Loss: 0.017654841765761375\n",
            "Epoch 8, Batch 140, Loss: 0.02034410834312439\n",
            "Epoch 8, Batch 150, Loss: 0.027330342680215836\n",
            "Epoch 8, Batch 160, Loss: 0.01956980675458908\n",
            "Epoch 8, Batch 170, Loss: 0.02136886678636074\n",
            "Epoch 8, Batch 180, Loss: 0.03147876262664795\n",
            "Epoch 8, Batch 190, Loss: 0.021838637068867683\n",
            "Epoch 8, Batch 200, Loss: 0.021765902638435364\n",
            "Epoch 8, Batch 210, Loss: 0.02146710641682148\n",
            "Epoch 8, Batch 220, Loss: 0.021215708926320076\n",
            "Epoch 8, Batch 230, Loss: 0.021284326910972595\n",
            "Epoch 8, Batch 240, Loss: 0.019295737147331238\n",
            "Epoch 8, Batch 250, Loss: 0.01722310483455658\n",
            "Epoch 8, Batch 260, Loss: 0.019069721922278404\n",
            "Epoch 8, Batch 270, Loss: 0.018074113875627518\n",
            "Epoch 8, Batch 280, Loss: 0.017976123839616776\n",
            "Epoch 8, Batch 290, Loss: 0.022543156519532204\n",
            "Epoch 8, Batch 300, Loss: 0.025428788736462593\n",
            "Epoch 8, Batch 310, Loss: 0.018542278558015823\n",
            "Epoch 8, Batch 320, Loss: 0.01948016695678234\n",
            "Epoch 8, Batch 330, Loss: 0.018687866628170013\n",
            "Epoch 8, Batch 340, Loss: 0.030334485694766045\n",
            "Epoch 8, Batch 350, Loss: 0.020205238834023476\n",
            "Epoch 8, Batch 360, Loss: 0.03084462694823742\n",
            "Epoch 8, Batch 370, Loss: 0.02040320634841919\n",
            "Epoch 8, Batch 380, Loss: 0.02309592440724373\n",
            "Epoch 8, Batch 390, Loss: 0.021307045593857765\n",
            "Epoch 8, Batch 400, Loss: 0.024469953030347824\n",
            "Epoch 8, Batch 410, Loss: 0.026173867285251617\n",
            "Epoch 8, Batch 420, Loss: 0.016953792423009872\n",
            "Epoch 8, Batch 430, Loss: 0.022983349859714508\n",
            "Epoch 8, Batch 440, Loss: 0.01889944262802601\n",
            "Epoch 8, Batch 450, Loss: 0.019563008099794388\n",
            "Epoch 8, Batch 460, Loss: 0.01642349548637867\n",
            "Epoch 8, Batch 470, Loss: 0.018153121694922447\n",
            "Epoch 8, Batch 480, Loss: 0.024632884189486504\n",
            "Epoch 8, Batch 490, Loss: 0.022267475724220276\n",
            "Epoch 8, Batch 500, Loss: 0.01933864876627922\n",
            "Epoch 8, Batch 510, Loss: 0.024505209177732468\n",
            "Epoch 8, Batch 520, Loss: 0.023809395730495453\n",
            "Epoch 8, Batch 530, Loss: 0.012573663145303726\n",
            "Epoch 8, Batch 540, Loss: 0.025426549836993217\n",
            "Epoch 8, Batch 550, Loss: 0.02711084485054016\n",
            "Epoch 8, Batch 560, Loss: 0.02189738303422928\n",
            "Epoch 8, Batch 570, Loss: 0.019307712092995644\n",
            "Epoch 8, Batch 580, Loss: 0.028683312237262726\n",
            "Epoch 8, Batch 590, Loss: 0.026503846049308777\n",
            "Epoch 8, Batch 600, Loss: 0.03374604135751724\n",
            "Epoch 8, Batch 610, Loss: 0.02603970281779766\n",
            "Epoch 8, Batch 620, Loss: 0.021685894578695297\n",
            "Epoch 8, Batch 630, Loss: 0.02485525980591774\n",
            "Epoch 8, Batch 640, Loss: 0.017396677285432816\n",
            "Epoch 8, Batch 650, Loss: 0.01955718919634819\n",
            "Epoch 8, Batch 660, Loss: 0.019263409078121185\n",
            "Epoch 8, Batch 670, Loss: 0.019128762185573578\n",
            "Epoch 8, Batch 680, Loss: 0.02143617533147335\n",
            "Epoch 8, Batch 690, Loss: 0.01836896687746048\n",
            "Epoch 8, Batch 700, Loss: 0.029755346477031708\n",
            "Epoch 8, Batch 710, Loss: 0.018356597051024437\n",
            "Epoch 9, Batch 0, Loss: 0.023231854662299156\n",
            "Epoch 9, Batch 10, Loss: 0.01973983645439148\n",
            "Epoch 9, Batch 20, Loss: 0.01861923187971115\n",
            "Epoch 9, Batch 30, Loss: 0.014160423539578915\n",
            "Epoch 9, Batch 40, Loss: 0.029856514185667038\n",
            "Epoch 9, Batch 50, Loss: 0.017105653882026672\n",
            "Epoch 9, Batch 60, Loss: 0.019784221425652504\n",
            "Epoch 9, Batch 70, Loss: 0.020935965701937675\n",
            "Epoch 9, Batch 80, Loss: 0.02109289914369583\n",
            "Epoch 9, Batch 90, Loss: 0.025902241468429565\n",
            "Epoch 9, Batch 100, Loss: 0.020139673724770546\n",
            "Epoch 9, Batch 110, Loss: 0.025830978527665138\n",
            "Epoch 9, Batch 120, Loss: 0.013428114354610443\n",
            "Epoch 9, Batch 130, Loss: 0.021714691072702408\n",
            "Epoch 9, Batch 140, Loss: 0.018979396671056747\n",
            "Epoch 9, Batch 150, Loss: 0.015263601206243038\n",
            "Epoch 9, Batch 160, Loss: 0.0172891728579998\n",
            "Epoch 9, Batch 170, Loss: 0.01659369468688965\n",
            "Epoch 9, Batch 180, Loss: 0.02373349294066429\n",
            "Epoch 9, Batch 190, Loss: 0.019184628501534462\n",
            "Epoch 9, Batch 200, Loss: 0.02206166461110115\n",
            "Epoch 9, Batch 210, Loss: 0.01313835196197033\n",
            "Epoch 9, Batch 220, Loss: 0.019914153963327408\n",
            "Epoch 9, Batch 230, Loss: 0.028571702539920807\n",
            "Epoch 9, Batch 240, Loss: 0.026284344494342804\n",
            "Epoch 9, Batch 250, Loss: 0.021634520962834358\n",
            "Epoch 9, Batch 260, Loss: 0.02228553779423237\n",
            "Epoch 9, Batch 270, Loss: 0.021309804171323776\n",
            "Epoch 9, Batch 280, Loss: 0.02339504100382328\n",
            "Epoch 9, Batch 290, Loss: 0.02368408814072609\n",
            "Epoch 9, Batch 300, Loss: 0.014167667366564274\n",
            "Epoch 9, Batch 310, Loss: 0.018401194363832474\n",
            "Epoch 9, Batch 320, Loss: 0.025516729801893234\n",
            "Epoch 9, Batch 330, Loss: 0.022333238273859024\n",
            "Epoch 9, Batch 340, Loss: 0.023171253502368927\n",
            "Epoch 9, Batch 350, Loss: 0.012580693699419498\n",
            "Epoch 9, Batch 360, Loss: 0.018718233332037926\n",
            "Epoch 9, Batch 370, Loss: 0.020451383665204048\n",
            "Epoch 9, Batch 380, Loss: 0.020834656432271004\n",
            "Epoch 9, Batch 390, Loss: 0.01687992364168167\n",
            "Epoch 9, Batch 400, Loss: 0.020337533205747604\n",
            "Epoch 9, Batch 410, Loss: 0.022183900699019432\n",
            "Epoch 9, Batch 420, Loss: 0.01690000295639038\n",
            "Epoch 9, Batch 430, Loss: 0.020387321710586548\n",
            "Epoch 9, Batch 440, Loss: 0.012723452411592007\n",
            "Epoch 9, Batch 450, Loss: 0.024209776893258095\n",
            "Epoch 9, Batch 460, Loss: 0.020249975845217705\n",
            "Epoch 9, Batch 470, Loss: 0.016016799956560135\n",
            "Epoch 9, Batch 480, Loss: 0.020845115184783936\n",
            "Epoch 9, Batch 490, Loss: 0.018003247678279877\n",
            "Epoch 9, Batch 500, Loss: 0.019318997859954834\n",
            "Epoch 9, Batch 510, Loss: 0.01647372357547283\n",
            "Epoch 9, Batch 520, Loss: 0.028716284781694412\n",
            "Epoch 9, Batch 530, Loss: 0.023676563054323196\n",
            "Epoch 9, Batch 540, Loss: 0.014609772711992264\n",
            "Epoch 9, Batch 550, Loss: 0.019767263904213905\n",
            "Epoch 9, Batch 560, Loss: 0.017985578626394272\n",
            "Epoch 9, Batch 570, Loss: 0.021627692505717278\n",
            "Epoch 9, Batch 580, Loss: 0.01927635259926319\n",
            "Epoch 9, Batch 590, Loss: 0.027781067416071892\n",
            "Epoch 9, Batch 600, Loss: 0.011088451370596886\n",
            "Epoch 9, Batch 610, Loss: 0.02241629920899868\n",
            "Epoch 9, Batch 620, Loss: 0.013802786357700825\n",
            "Epoch 9, Batch 630, Loss: 0.025385495275259018\n",
            "Epoch 9, Batch 640, Loss: 0.013954936526715755\n",
            "Epoch 9, Batch 650, Loss: 0.03101956844329834\n",
            "Epoch 9, Batch 660, Loss: 0.028310148045420647\n",
            "Epoch 9, Batch 670, Loss: 0.019038870930671692\n",
            "Epoch 9, Batch 680, Loss: 0.014103876426815987\n",
            "Epoch 9, Batch 690, Loss: 0.020868990570306778\n",
            "Epoch 9, Batch 700, Loss: 0.03034413978457451\n",
            "Epoch 9, Batch 710, Loss: 0.019377078860998154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test dataset\n",
        "model.eval()\n",
        "# Define test dataset\n",
        "test_dataset = TensorDataset(test_user, test_course, test_grade)\n",
        "# Define test dataloader\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "total_test_loss = 0.0\n",
        "num_test_samples = len(test_dataset)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        user,course, grade = batch\n",
        "        output = model(course).squeeze()\n",
        "        loss = criterion(output, grade)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "test_mse = total_test_loss / num_test_samples\n",
        "print(f'Mean Squared Error (MSE) on the test dataset: {test_mse}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akciIQj_88LQ",
        "outputId": "9f928f58-4b07-427f-eeb4-a2561b6a1933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) on the test dataset: 0.000335361745078151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommender function modified to exclude user_id\n",
        "def recommend_courses(model, input_courses, top_k):\n",
        "    # Convert input_courses to PyTorch tensor\n",
        "    input_course_ids = [le_course.transform([course])[0] for course in input_courses.keys()]\n",
        "    input_course_grades = list(input_courses.values())\n",
        "\n",
        "    # Make predictions for all courses\n",
        "    all_course_ids = torch.arange(len(le_course.classes_))\n",
        "    predictions = model(all_course_ids).squeeze()\n",
        "\n",
        "    # Exclude input courses from recommendations\n",
        "    for course_id in input_course_ids:\n",
        "        predictions[course_id] = float('-inf')\n",
        "\n",
        "    # Get the indices of the top-k predictions\n",
        "    num_recommendations = min(top_k, len(predictions))\n",
        "    top_indices = torch.topk(predictions, num_recommendations).indices\n",
        "\n",
        "    # Map the top indices back to the course IDs\n",
        "    top_course_ids = le_course.inverse_transform(top_indices.numpy())\n",
        "\n",
        "    # Exclude input courses from recommendations (additional check)\n",
        "    top_course_ids = [course_id for course_id in top_course_ids if course_id not in input_courses]\n",
        "\n",
        "    return top_course_ids\n"
      ],
      "metadata": {
        "id": "8caJJ0SpvQHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Input student's grade for previous courses\n",
        "input_courses = {\n",
        "    'CHEMISTRY LABORATORY': 8,\n",
        "    'GENERAL CHEMISTRY': 10,\n",
        "    'ELECTRICAL SCIENCES':10,\n",
        "    'ADDITIVE MANUFACTURING': 10,\n",
        "    'PRACTICE SCHOOL I':10,\n",
        "    'PHYSICS LABORATORY':10\n",
        "}\n",
        "\n",
        "# Set top_k to the desired number\n",
        "top_k = 5\n",
        "\n",
        "# Call the function with the updated course grades\n",
        "recommended_courses = recommend_courses(model, input_courses, top_k)\n",
        "\n",
        "print(f\"Top {top_k} recommended courses based on previous grades: {recommended_courses}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ksrILqm9BGP",
        "outputId": "ee961a06-17fc-48dc-dea5-4452d8a1efef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommended courses based on previous grades: ['PRACTICE SCHOOL II', 'ELEC MAGNET & OPTICS LAB', 'THESIS', 'ELEC & ELECTRONIC CIRCUITS LAB', 'LABORATORY PROJECT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svwqqnja9C8B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}