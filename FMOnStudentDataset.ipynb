{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishnu0920/Recommender_System_Models/blob/main/FMOnStudentDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-VWMFAVs_yl",
        "outputId": "b9143fc2-d203-4326-90df-dabdd11eef93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting pyfm\n",
            "  Downloading pyfm-0.2.4.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyfm) (2.31.0)\n",
            "Collecting urwid>=1.2.1 (from pyfm)\n",
            "  Downloading urwid-2.6.9-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.1/296.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->pyfm) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from urwid>=1.2.1->pyfm) (4.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from urwid>=1.2.1->pyfm) (0.2.13)\n",
            "Building wheels for collected packages: pyfm\n",
            "  Building wheel for pyfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfm: filename=pyfm-0.2.4-py3-none-any.whl size=13208 sha256=a500f8ed3f6ad5b6a36e626bbc4de7ef21d01f5398840d55f7e1fb030f64cf77\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/5e/81/2218255b6118f1373b7b77b2062d8d2e7f757186cedca7becc\n",
            "Successfully built pyfm\n",
            "Installing collected packages: urwid, pyfm\n",
            "Successfully installed pyfm-0.2.4 urwid-2.6.9\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas torch scikit-learn\n",
        "!pip install pyfm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoHbdTgty_Gw"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Import libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hajnNzKzzfgf"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Load the dataset\n",
        "df = pd.read_csv('grade_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64G_q5n60Gsj"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Encode student_id and course_id using LabelEncoder\n",
        "le_student = LabelEncoder()\n",
        "le_course = LabelEncoder()\n",
        "\n",
        "df['student_id'] = le_student.fit_transform(df['student_id'])\n",
        "df['course_id'] = le_course.fit_transform(df['course_id'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei0zH-Lv0OfY"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Map course grades to the range [0, 1] for regression\n",
        "df['course_grade'] = df['course_grade'] / 10.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ClpboVC1eLX"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Train-test split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0R7rTcU1jru"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Convert the data to PyTorch tensors\n",
        "train_user = torch.LongTensor(train_df['student_id'].values)\n",
        "train_course = torch.LongTensor(train_df['course_id'].values)\n",
        "train_grade = torch.FloatTensor(train_df['course_grade'].values)\n",
        "\n",
        "test_user = torch.LongTensor(test_df['student_id'].values)\n",
        "test_course = torch.LongTensor(test_df['course_id'].values)\n",
        "test_grade = torch.FloatTensor(test_df['course_grade'].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hpx8-aib1p4y"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Define the Factorization Machine model\n",
        "class FactorizationMachine(nn.Module):\n",
        "    def __init__(self, num_users, num_courses, embedding_dim):\n",
        "        super(FactorizationMachine, self).__init__()\n",
        "\n",
        "        # Adjust the dimensions of embeddings and linear layer\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.course_embedding = nn.Embedding(num_courses, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim * 2, 1)\n",
        "\n",
        "    def forward(self, user, course):\n",
        "        user_emb = self.user_embedding(user)\n",
        "        course_emb = self.course_embedding(course)\n",
        "\n",
        "        # Concatenate user and course embeddings along the last dimension\n",
        "        interaction = torch.cat([user_emb, course_emb], dim=1)\n",
        "\n",
        "        output = self.linear(interaction)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPgBbyCkX5EC"
      },
      "outputs": [],
      "source": [
        "# class FactorizationMachine(nn.Module):\n",
        "#     def forward(self, user, course):\n",
        "#         user_emb = self.user_embedding(user)\n",
        "#         course_emb = self.course_embedding(course)\n",
        "\n",
        "#         # Reshape embeddings if necessary\n",
        "#         user_emb = user_emb.unsqueeze(1) if len(user_emb.shape) < 3 else user_emb\n",
        "#         course_emb = course_emb.unsqueeze(1) if len(course_emb.shape) < 3 else course_emb\n",
        "\n",
        "#         # Concatenate user and course embeddings along the last dimension\n",
        "#         interaction = torch.cat([user_emb, course_emb], dim=2)\n",
        "\n",
        "#         # Reshape interaction tensor\n",
        "#         interaction = interaction.view(-1, interaction.size(1) * interaction.size(2))\n",
        "\n",
        "#         output = self.linear(interaction)\n",
        "#         return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7liq0m3T1ta9"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Hyperparameters\n",
        "num_users = len(le_student.classes_)\n",
        "num_courses = len(le_course.classes_)\n",
        "embedding_dim = 10\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aFcgBuf2kuc"
      },
      "outputs": [],
      "source": [
        "# Cell 10: Create DataLoader for training\n",
        "train_dataset = TensorDataset(train_user, train_course, train_grade)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiEi26kZ2pt9"
      },
      "outputs": [],
      "source": [
        "# Cell 11: Instantiate the model, define loss function and optimizer\n",
        "model = FactorizationMachine(num_users, num_courses, embedding_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vxgCfb32t3-",
        "outputId": "6aa85adc-9795-4af8-baf5-c49834aa264b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch 0, Loss: 0.8195591568946838\n",
            "Epoch 0, Batch 10, Loss: 0.5981851816177368\n",
            "Epoch 0, Batch 20, Loss: 0.514411211013794\n",
            "Epoch 0, Batch 30, Loss: 0.4948277771472931\n",
            "Epoch 0, Batch 40, Loss: 0.5008884072303772\n",
            "Epoch 0, Batch 50, Loss: 0.4669727385044098\n",
            "Epoch 0, Batch 60, Loss: 0.4339965283870697\n",
            "Epoch 0, Batch 70, Loss: 0.35265734791755676\n",
            "Epoch 0, Batch 80, Loss: 0.4525752067565918\n",
            "Epoch 0, Batch 90, Loss: 0.320129930973053\n",
            "Epoch 0, Batch 100, Loss: 0.25728151202201843\n",
            "Epoch 0, Batch 110, Loss: 0.28497493267059326\n",
            "Epoch 0, Batch 120, Loss: 0.2424587905406952\n",
            "Epoch 0, Batch 130, Loss: 0.2943894863128662\n",
            "Epoch 0, Batch 140, Loss: 0.2656079828739166\n",
            "Epoch 0, Batch 150, Loss: 0.20164929330348969\n",
            "Epoch 0, Batch 160, Loss: 0.1889808177947998\n",
            "Epoch 0, Batch 170, Loss: 0.20459142327308655\n",
            "Epoch 0, Batch 180, Loss: 0.20902928709983826\n",
            "Epoch 0, Batch 190, Loss: 0.1690414845943451\n",
            "Epoch 0, Batch 200, Loss: 0.1974780410528183\n",
            "Epoch 0, Batch 210, Loss: 0.1615159958600998\n",
            "Epoch 0, Batch 220, Loss: 0.1357329785823822\n",
            "Epoch 0, Batch 230, Loss: 0.15148736536502838\n",
            "Epoch 0, Batch 240, Loss: 0.12417112290859222\n",
            "Epoch 0, Batch 250, Loss: 0.14287719130516052\n",
            "Epoch 0, Batch 260, Loss: 0.14578956365585327\n",
            "Epoch 0, Batch 270, Loss: 0.14466239511966705\n",
            "Epoch 0, Batch 280, Loss: 0.11445426940917969\n",
            "Epoch 0, Batch 290, Loss: 0.10679572075605392\n",
            "Epoch 0, Batch 300, Loss: 0.11785513162612915\n",
            "Epoch 0, Batch 310, Loss: 0.0987902283668518\n",
            "Epoch 0, Batch 320, Loss: 0.09384667873382568\n",
            "Epoch 0, Batch 330, Loss: 0.10391709953546524\n",
            "Epoch 0, Batch 340, Loss: 0.09630858153104782\n",
            "Epoch 0, Batch 350, Loss: 0.0829450711607933\n",
            "Epoch 0, Batch 360, Loss: 0.09742471575737\n",
            "Epoch 0, Batch 370, Loss: 0.08375519514083862\n",
            "Epoch 0, Batch 380, Loss: 0.08740109205245972\n",
            "Epoch 0, Batch 390, Loss: 0.06566637754440308\n",
            "Epoch 0, Batch 400, Loss: 0.07822176814079285\n",
            "Epoch 0, Batch 410, Loss: 0.06983631104230881\n",
            "Epoch 0, Batch 420, Loss: 0.07727733254432678\n",
            "Epoch 0, Batch 430, Loss: 0.06672448664903641\n",
            "Epoch 0, Batch 440, Loss: 0.06870688498020172\n",
            "Epoch 0, Batch 450, Loss: 0.09433168172836304\n",
            "Epoch 0, Batch 460, Loss: 0.056450873613357544\n",
            "Epoch 0, Batch 470, Loss: 0.060219261795282364\n",
            "Epoch 0, Batch 480, Loss: 0.04940478131175041\n",
            "Epoch 0, Batch 490, Loss: 0.06018500775098801\n",
            "Epoch 0, Batch 500, Loss: 0.058455660939216614\n",
            "Epoch 0, Batch 510, Loss: 0.05237679183483124\n",
            "Epoch 0, Batch 520, Loss: 0.04818768426775932\n",
            "Epoch 0, Batch 530, Loss: 0.04569539427757263\n",
            "Epoch 0, Batch 540, Loss: 0.040042344480752945\n",
            "Epoch 0, Batch 550, Loss: 0.046409860253334045\n",
            "Epoch 0, Batch 560, Loss: 0.035894982516765594\n",
            "Epoch 0, Batch 570, Loss: 0.052497509866952896\n",
            "Epoch 0, Batch 580, Loss: 0.05008696764707565\n",
            "Epoch 0, Batch 590, Loss: 0.043487947434186935\n",
            "Epoch 0, Batch 600, Loss: 0.0323689766228199\n",
            "Epoch 0, Batch 610, Loss: 0.040092308074235916\n",
            "Epoch 0, Batch 620, Loss: 0.04468560218811035\n",
            "Epoch 0, Batch 630, Loss: 0.0454622358083725\n",
            "Epoch 0, Batch 640, Loss: 0.03087673895061016\n",
            "Epoch 0, Batch 650, Loss: 0.05014050006866455\n",
            "Epoch 0, Batch 660, Loss: 0.03093348629772663\n",
            "Epoch 0, Batch 670, Loss: 0.051240816712379456\n",
            "Epoch 0, Batch 680, Loss: 0.038238804787397385\n",
            "Epoch 0, Batch 690, Loss: 0.026835957542061806\n",
            "Epoch 0, Batch 700, Loss: 0.027484752237796783\n",
            "Epoch 0, Batch 710, Loss: 0.02612689882516861\n",
            "Epoch 1, Batch 0, Loss: 0.03645487129688263\n",
            "Epoch 1, Batch 10, Loss: 0.03217029571533203\n",
            "Epoch 1, Batch 20, Loss: 0.040251459926366806\n",
            "Epoch 1, Batch 30, Loss: 0.04557988792657852\n",
            "Epoch 1, Batch 40, Loss: 0.031401947140693665\n",
            "Epoch 1, Batch 50, Loss: 0.031039869412779808\n",
            "Epoch 1, Batch 60, Loss: 0.025375185534358025\n",
            "Epoch 1, Batch 70, Loss: 0.03036203607916832\n",
            "Epoch 1, Batch 80, Loss: 0.03245507925748825\n",
            "Epoch 1, Batch 90, Loss: 0.027431970462203026\n",
            "Epoch 1, Batch 100, Loss: 0.027520105242729187\n",
            "Epoch 1, Batch 110, Loss: 0.040934912860393524\n",
            "Epoch 1, Batch 120, Loss: 0.024756843224167824\n",
            "Epoch 1, Batch 130, Loss: 0.033269863575696945\n",
            "Epoch 1, Batch 140, Loss: 0.029916144907474518\n",
            "Epoch 1, Batch 150, Loss: 0.023701464757323265\n",
            "Epoch 1, Batch 160, Loss: 0.03486854210495949\n",
            "Epoch 1, Batch 170, Loss: 0.037908948957920074\n",
            "Epoch 1, Batch 180, Loss: 0.020380768924951553\n",
            "Epoch 1, Batch 190, Loss: 0.02949519455432892\n",
            "Epoch 1, Batch 200, Loss: 0.029450733214616776\n",
            "Epoch 1, Batch 210, Loss: 0.018807770684361458\n",
            "Epoch 1, Batch 220, Loss: 0.029322922229766846\n",
            "Epoch 1, Batch 230, Loss: 0.032477810978889465\n",
            "Epoch 1, Batch 240, Loss: 0.02768564037978649\n",
            "Epoch 1, Batch 250, Loss: 0.029567833989858627\n",
            "Epoch 1, Batch 260, Loss: 0.025293147191405296\n",
            "Epoch 1, Batch 270, Loss: 0.040340472012758255\n",
            "Epoch 1, Batch 280, Loss: 0.022886015474796295\n",
            "Epoch 1, Batch 290, Loss: 0.03268587961792946\n",
            "Epoch 1, Batch 300, Loss: 0.026005512103438377\n",
            "Epoch 1, Batch 310, Loss: 0.02775491029024124\n",
            "Epoch 1, Batch 320, Loss: 0.024112027138471603\n",
            "Epoch 1, Batch 330, Loss: 0.04478298872709274\n",
            "Epoch 1, Batch 340, Loss: 0.027314752340316772\n",
            "Epoch 1, Batch 350, Loss: 0.03449187055230141\n",
            "Epoch 1, Batch 360, Loss: 0.022445155307650566\n",
            "Epoch 1, Batch 370, Loss: 0.0233341995626688\n",
            "Epoch 1, Batch 380, Loss: 0.02712969295680523\n",
            "Epoch 1, Batch 390, Loss: 0.030621971935033798\n",
            "Epoch 1, Batch 400, Loss: 0.03162073716521263\n",
            "Epoch 1, Batch 410, Loss: 0.023886999115347862\n",
            "Epoch 1, Batch 420, Loss: 0.034230999648571014\n",
            "Epoch 1, Batch 430, Loss: 0.027051091194152832\n",
            "Epoch 1, Batch 440, Loss: 0.025494255125522614\n",
            "Epoch 1, Batch 450, Loss: 0.01714513823390007\n",
            "Epoch 1, Batch 460, Loss: 0.034400925040245056\n",
            "Epoch 1, Batch 470, Loss: 0.01703953742980957\n",
            "Epoch 1, Batch 480, Loss: 0.033088989555835724\n",
            "Epoch 1, Batch 490, Loss: 0.030529502779245377\n",
            "Epoch 1, Batch 500, Loss: 0.03452450409531593\n",
            "Epoch 1, Batch 510, Loss: 0.02135157771408558\n",
            "Epoch 1, Batch 520, Loss: 0.03238819167017937\n",
            "Epoch 1, Batch 530, Loss: 0.0323689728975296\n",
            "Epoch 1, Batch 540, Loss: 0.024500880390405655\n",
            "Epoch 1, Batch 550, Loss: 0.027318749576807022\n",
            "Epoch 1, Batch 560, Loss: 0.03276009485125542\n",
            "Epoch 1, Batch 570, Loss: 0.0320141464471817\n",
            "Epoch 1, Batch 580, Loss: 0.03003377467393875\n",
            "Epoch 1, Batch 590, Loss: 0.026426926255226135\n",
            "Epoch 1, Batch 600, Loss: 0.03211985528469086\n",
            "Epoch 1, Batch 610, Loss: 0.021239783614873886\n",
            "Epoch 1, Batch 620, Loss: 0.01991375908255577\n",
            "Epoch 1, Batch 630, Loss: 0.03532307222485542\n",
            "Epoch 1, Batch 640, Loss: 0.029212241992354393\n",
            "Epoch 1, Batch 650, Loss: 0.025203507393598557\n",
            "Epoch 1, Batch 660, Loss: 0.01949886605143547\n",
            "Epoch 1, Batch 670, Loss: 0.03170100599527359\n",
            "Epoch 1, Batch 680, Loss: 0.042952749878168106\n",
            "Epoch 1, Batch 690, Loss: 0.02287413738667965\n",
            "Epoch 1, Batch 700, Loss: 0.024798713624477386\n",
            "Epoch 1, Batch 710, Loss: 0.03208775818347931\n",
            "Epoch 2, Batch 0, Loss: 0.03391548991203308\n",
            "Epoch 2, Batch 10, Loss: 0.03078598529100418\n",
            "Epoch 2, Batch 20, Loss: 0.021852783858776093\n",
            "Epoch 2, Batch 30, Loss: 0.030626825988292694\n",
            "Epoch 2, Batch 40, Loss: 0.01700558140873909\n",
            "Epoch 2, Batch 50, Loss: 0.021590836346149445\n",
            "Epoch 2, Batch 60, Loss: 0.023771677166223526\n",
            "Epoch 2, Batch 70, Loss: 0.028657792136073112\n",
            "Epoch 2, Batch 80, Loss: 0.024226948618888855\n",
            "Epoch 2, Batch 90, Loss: 0.03465059772133827\n",
            "Epoch 2, Batch 100, Loss: 0.027112986892461777\n",
            "Epoch 2, Batch 110, Loss: 0.02637186460196972\n",
            "Epoch 2, Batch 120, Loss: 0.029401633888483047\n",
            "Epoch 2, Batch 130, Loss: 0.02398194745182991\n",
            "Epoch 2, Batch 140, Loss: 0.0207901019603014\n",
            "Epoch 2, Batch 150, Loss: 0.02495027892291546\n",
            "Epoch 2, Batch 160, Loss: 0.033599063754081726\n",
            "Epoch 2, Batch 170, Loss: 0.0316162146627903\n",
            "Epoch 2, Batch 180, Loss: 0.02955850027501583\n",
            "Epoch 2, Batch 190, Loss: 0.03288725018501282\n",
            "Epoch 2, Batch 200, Loss: 0.024881938472390175\n",
            "Epoch 2, Batch 210, Loss: 0.02679164707660675\n",
            "Epoch 2, Batch 220, Loss: 0.03200546279549599\n",
            "Epoch 2, Batch 230, Loss: 0.02122923731803894\n",
            "Epoch 2, Batch 240, Loss: 0.028371736407279968\n",
            "Epoch 2, Batch 250, Loss: 0.02182915434241295\n",
            "Epoch 2, Batch 260, Loss: 0.0251145139336586\n",
            "Epoch 2, Batch 270, Loss: 0.0202883742749691\n",
            "Epoch 2, Batch 280, Loss: 0.023856043815612793\n",
            "Epoch 2, Batch 290, Loss: 0.0284979660063982\n",
            "Epoch 2, Batch 300, Loss: 0.024881847202777863\n",
            "Epoch 2, Batch 310, Loss: 0.025299282744526863\n",
            "Epoch 2, Batch 320, Loss: 0.03391724452376366\n",
            "Epoch 2, Batch 330, Loss: 0.03662298247218132\n",
            "Epoch 2, Batch 340, Loss: 0.02779422327876091\n",
            "Epoch 2, Batch 350, Loss: 0.02161942608654499\n",
            "Epoch 2, Batch 360, Loss: 0.035596705973148346\n",
            "Epoch 2, Batch 370, Loss: 0.02777179144322872\n",
            "Epoch 2, Batch 380, Loss: 0.025349833071231842\n",
            "Epoch 2, Batch 390, Loss: 0.026284748688340187\n",
            "Epoch 2, Batch 400, Loss: 0.03235751390457153\n",
            "Epoch 2, Batch 410, Loss: 0.02457932010293007\n",
            "Epoch 2, Batch 420, Loss: 0.02318647690117359\n",
            "Epoch 2, Batch 430, Loss: 0.03194725513458252\n",
            "Epoch 2, Batch 440, Loss: 0.02595621533691883\n",
            "Epoch 2, Batch 450, Loss: 0.027454141527414322\n",
            "Epoch 2, Batch 460, Loss: 0.03357153385877609\n",
            "Epoch 2, Batch 470, Loss: 0.025171594694256783\n",
            "Epoch 2, Batch 480, Loss: 0.020425042137503624\n",
            "Epoch 2, Batch 490, Loss: 0.018926002085208893\n",
            "Epoch 2, Batch 500, Loss: 0.018299110233783722\n",
            "Epoch 2, Batch 510, Loss: 0.025197666138410568\n",
            "Epoch 2, Batch 520, Loss: 0.033364374190568924\n",
            "Epoch 2, Batch 530, Loss: 0.02824968844652176\n",
            "Epoch 2, Batch 540, Loss: 0.02503162994980812\n",
            "Epoch 2, Batch 550, Loss: 0.020775670185685158\n",
            "Epoch 2, Batch 560, Loss: 0.029018927365541458\n",
            "Epoch 2, Batch 570, Loss: 0.03242915868759155\n",
            "Epoch 2, Batch 580, Loss: 0.02605539746582508\n",
            "Epoch 2, Batch 590, Loss: 0.026651959866285324\n",
            "Epoch 2, Batch 600, Loss: 0.02640148438513279\n",
            "Epoch 2, Batch 610, Loss: 0.024871308356523514\n",
            "Epoch 2, Batch 620, Loss: 0.022273389622569084\n",
            "Epoch 2, Batch 630, Loss: 0.02946973778307438\n",
            "Epoch 2, Batch 640, Loss: 0.030128009617328644\n",
            "Epoch 2, Batch 650, Loss: 0.03160543739795685\n",
            "Epoch 2, Batch 660, Loss: 0.025869393721222878\n",
            "Epoch 2, Batch 670, Loss: 0.023404691368341446\n",
            "Epoch 2, Batch 680, Loss: 0.022339530289173126\n",
            "Epoch 2, Batch 690, Loss: 0.03656614199280739\n",
            "Epoch 2, Batch 700, Loss: 0.02533339336514473\n",
            "Epoch 2, Batch 710, Loss: 0.025427356362342834\n",
            "Epoch 3, Batch 0, Loss: 0.02086057886481285\n",
            "Epoch 3, Batch 10, Loss: 0.02609984204173088\n",
            "Epoch 3, Batch 20, Loss: 0.03172759339213371\n",
            "Epoch 3, Batch 30, Loss: 0.026572981849312782\n",
            "Epoch 3, Batch 40, Loss: 0.017460158094763756\n",
            "Epoch 3, Batch 50, Loss: 0.02274593710899353\n",
            "Epoch 3, Batch 60, Loss: 0.027861136943101883\n",
            "Epoch 3, Batch 70, Loss: 0.029192455112934113\n",
            "Epoch 3, Batch 80, Loss: 0.035127464681863785\n",
            "Epoch 3, Batch 90, Loss: 0.03239401429891586\n",
            "Epoch 3, Batch 100, Loss: 0.029337342828512192\n",
            "Epoch 3, Batch 110, Loss: 0.021605024114251137\n",
            "Epoch 3, Batch 120, Loss: 0.022238552570343018\n",
            "Epoch 3, Batch 130, Loss: 0.02496626228094101\n",
            "Epoch 3, Batch 140, Loss: 0.02920352667570114\n",
            "Epoch 3, Batch 150, Loss: 0.023866577073931694\n",
            "Epoch 3, Batch 160, Loss: 0.022816861048340797\n",
            "Epoch 3, Batch 170, Loss: 0.02190602570772171\n",
            "Epoch 3, Batch 180, Loss: 0.017473794519901276\n",
            "Epoch 3, Batch 190, Loss: 0.024897653609514236\n",
            "Epoch 3, Batch 200, Loss: 0.02130473032593727\n",
            "Epoch 3, Batch 210, Loss: 0.02087673544883728\n",
            "Epoch 3, Batch 220, Loss: 0.02141965925693512\n",
            "Epoch 3, Batch 230, Loss: 0.027481254190206528\n",
            "Epoch 3, Batch 240, Loss: 0.03749162703752518\n",
            "Epoch 3, Batch 250, Loss: 0.018993278965353966\n",
            "Epoch 3, Batch 260, Loss: 0.02293551340699196\n",
            "Epoch 3, Batch 270, Loss: 0.01903589256107807\n",
            "Epoch 3, Batch 280, Loss: 0.022125937044620514\n",
            "Epoch 3, Batch 290, Loss: 0.027566730976104736\n",
            "Epoch 3, Batch 300, Loss: 0.029355917125940323\n",
            "Epoch 3, Batch 310, Loss: 0.0164451003074646\n",
            "Epoch 3, Batch 320, Loss: 0.021262705326080322\n",
            "Epoch 3, Batch 330, Loss: 0.016104400157928467\n",
            "Epoch 3, Batch 340, Loss: 0.031773246824741364\n",
            "Epoch 3, Batch 350, Loss: 0.019574327394366264\n",
            "Epoch 3, Batch 360, Loss: 0.02910565212368965\n",
            "Epoch 3, Batch 370, Loss: 0.025911908596754074\n",
            "Epoch 3, Batch 380, Loss: 0.025555061176419258\n",
            "Epoch 3, Batch 390, Loss: 0.017642859369516373\n",
            "Epoch 3, Batch 400, Loss: 0.0324624702334404\n",
            "Epoch 3, Batch 410, Loss: 0.0199014563113451\n",
            "Epoch 3, Batch 420, Loss: 0.02631649561226368\n",
            "Epoch 3, Batch 430, Loss: 0.02992810308933258\n",
            "Epoch 3, Batch 440, Loss: 0.0190037339925766\n",
            "Epoch 3, Batch 450, Loss: 0.023528767749667168\n",
            "Epoch 3, Batch 460, Loss: 0.03815534710884094\n",
            "Epoch 3, Batch 470, Loss: 0.023794615641236305\n",
            "Epoch 3, Batch 480, Loss: 0.027497485280036926\n",
            "Epoch 3, Batch 490, Loss: 0.02746448665857315\n",
            "Epoch 3, Batch 500, Loss: 0.025770775973796844\n",
            "Epoch 3, Batch 510, Loss: 0.026302075013518333\n",
            "Epoch 3, Batch 520, Loss: 0.03151480853557587\n",
            "Epoch 3, Batch 530, Loss: 0.02250106818974018\n",
            "Epoch 3, Batch 540, Loss: 0.01890866830945015\n",
            "Epoch 3, Batch 550, Loss: 0.021703165024518967\n",
            "Epoch 3, Batch 560, Loss: 0.023764407262206078\n",
            "Epoch 3, Batch 570, Loss: 0.025003965944051743\n",
            "Epoch 3, Batch 580, Loss: 0.032417964190244675\n",
            "Epoch 3, Batch 590, Loss: 0.017891909927129745\n",
            "Epoch 3, Batch 600, Loss: 0.027118954807519913\n",
            "Epoch 3, Batch 610, Loss: 0.02727915719151497\n",
            "Epoch 3, Batch 620, Loss: 0.02430371753871441\n",
            "Epoch 3, Batch 630, Loss: 0.027493348345160484\n",
            "Epoch 3, Batch 640, Loss: 0.03197657689452171\n",
            "Epoch 3, Batch 650, Loss: 0.01942421868443489\n",
            "Epoch 3, Batch 660, Loss: 0.021593857556581497\n",
            "Epoch 3, Batch 670, Loss: 0.01971389353275299\n",
            "Epoch 3, Batch 680, Loss: 0.01859554648399353\n",
            "Epoch 3, Batch 690, Loss: 0.01860278658568859\n",
            "Epoch 3, Batch 700, Loss: 0.026665478944778442\n",
            "Epoch 3, Batch 710, Loss: 0.024286659434437752\n",
            "Epoch 4, Batch 0, Loss: 0.02313975989818573\n",
            "Epoch 4, Batch 10, Loss: 0.03543899953365326\n",
            "Epoch 4, Batch 20, Loss: 0.01801934465765953\n",
            "Epoch 4, Batch 30, Loss: 0.02134675346314907\n",
            "Epoch 4, Batch 40, Loss: 0.02767164260149002\n",
            "Epoch 4, Batch 50, Loss: 0.02213616669178009\n",
            "Epoch 4, Batch 60, Loss: 0.018764398992061615\n",
            "Epoch 4, Batch 70, Loss: 0.018527647480368614\n",
            "Epoch 4, Batch 80, Loss: 0.026122145354747772\n",
            "Epoch 4, Batch 90, Loss: 0.018863605335354805\n",
            "Epoch 4, Batch 100, Loss: 0.025523962453007698\n",
            "Epoch 4, Batch 110, Loss: 0.01656104065477848\n",
            "Epoch 4, Batch 120, Loss: 0.025970416143536568\n",
            "Epoch 4, Batch 130, Loss: 0.016055159270763397\n",
            "Epoch 4, Batch 140, Loss: 0.01868286356329918\n",
            "Epoch 4, Batch 150, Loss: 0.031377118080854416\n",
            "Epoch 4, Batch 160, Loss: 0.027220018208026886\n",
            "Epoch 4, Batch 170, Loss: 0.021589072421193123\n",
            "Epoch 4, Batch 180, Loss: 0.030120745301246643\n",
            "Epoch 4, Batch 190, Loss: 0.018149064853787422\n",
            "Epoch 4, Batch 200, Loss: 0.017859993502497673\n",
            "Epoch 4, Batch 210, Loss: 0.01618243381381035\n",
            "Epoch 4, Batch 220, Loss: 0.020888758823275566\n",
            "Epoch 4, Batch 230, Loss: 0.030508078634738922\n",
            "Epoch 4, Batch 240, Loss: 0.016973398625850677\n",
            "Epoch 4, Batch 250, Loss: 0.018503833562135696\n",
            "Epoch 4, Batch 260, Loss: 0.01599612645804882\n",
            "Epoch 4, Batch 270, Loss: 0.023525945842266083\n",
            "Epoch 4, Batch 280, Loss: 0.02288251742720604\n",
            "Epoch 4, Batch 290, Loss: 0.03086302988231182\n",
            "Epoch 4, Batch 300, Loss: 0.020002132281661034\n",
            "Epoch 4, Batch 310, Loss: 0.019281331449747086\n",
            "Epoch 4, Batch 320, Loss: 0.030785702168941498\n",
            "Epoch 4, Batch 330, Loss: 0.02527758479118347\n",
            "Epoch 4, Batch 340, Loss: 0.017780307680368423\n",
            "Epoch 4, Batch 350, Loss: 0.026503419503569603\n",
            "Epoch 4, Batch 360, Loss: 0.03037453442811966\n",
            "Epoch 4, Batch 370, Loss: 0.020306771621108055\n",
            "Epoch 4, Batch 380, Loss: 0.021156638860702515\n",
            "Epoch 4, Batch 390, Loss: 0.020674176514148712\n",
            "Epoch 4, Batch 400, Loss: 0.029399583116173744\n",
            "Epoch 4, Batch 410, Loss: 0.024218030273914337\n",
            "Epoch 4, Batch 420, Loss: 0.018132684752345085\n",
            "Epoch 4, Batch 430, Loss: 0.015789875760674477\n",
            "Epoch 4, Batch 440, Loss: 0.02875097468495369\n",
            "Epoch 4, Batch 450, Loss: 0.02743845246732235\n",
            "Epoch 4, Batch 460, Loss: 0.03735556825995445\n",
            "Epoch 4, Batch 470, Loss: 0.018912792205810547\n",
            "Epoch 4, Batch 480, Loss: 0.016985880210995674\n",
            "Epoch 4, Batch 490, Loss: 0.03223974257707596\n",
            "Epoch 4, Batch 500, Loss: 0.018294023349881172\n",
            "Epoch 4, Batch 510, Loss: 0.015521235764026642\n",
            "Epoch 4, Batch 520, Loss: 0.023705551400780678\n",
            "Epoch 4, Batch 530, Loss: 0.017001574859023094\n",
            "Epoch 4, Batch 540, Loss: 0.02466569095849991\n",
            "Epoch 4, Batch 550, Loss: 0.02865489199757576\n",
            "Epoch 4, Batch 560, Loss: 0.022475460544228554\n",
            "Epoch 4, Batch 570, Loss: 0.01836618408560753\n",
            "Epoch 4, Batch 580, Loss: 0.018720950931310654\n",
            "Epoch 4, Batch 590, Loss: 0.014244627207517624\n",
            "Epoch 4, Batch 600, Loss: 0.02050808072090149\n",
            "Epoch 4, Batch 610, Loss: 0.02863895520567894\n",
            "Epoch 4, Batch 620, Loss: 0.02514619752764702\n",
            "Epoch 4, Batch 630, Loss: 0.025223836302757263\n",
            "Epoch 4, Batch 640, Loss: 0.021145865321159363\n",
            "Epoch 4, Batch 650, Loss: 0.025700164958834648\n",
            "Epoch 4, Batch 660, Loss: 0.02081940695643425\n",
            "Epoch 4, Batch 670, Loss: 0.021031176671385765\n",
            "Epoch 4, Batch 680, Loss: 0.013155417516827583\n",
            "Epoch 4, Batch 690, Loss: 0.022821204736828804\n",
            "Epoch 4, Batch 700, Loss: 0.027653709053993225\n",
            "Epoch 4, Batch 710, Loss: 0.018651552498340607\n",
            "Epoch 5, Batch 0, Loss: 0.015645861625671387\n",
            "Epoch 5, Batch 10, Loss: 0.030701521784067154\n",
            "Epoch 5, Batch 20, Loss: 0.024366114288568497\n",
            "Epoch 5, Batch 30, Loss: 0.021537549793720245\n",
            "Epoch 5, Batch 40, Loss: 0.02850976213812828\n",
            "Epoch 5, Batch 50, Loss: 0.02082875370979309\n",
            "Epoch 5, Batch 60, Loss: 0.01273388136178255\n",
            "Epoch 5, Batch 70, Loss: 0.02643546275794506\n",
            "Epoch 5, Batch 80, Loss: 0.02966928295791149\n",
            "Epoch 5, Batch 90, Loss: 0.021817201748490334\n",
            "Epoch 5, Batch 100, Loss: 0.01911919191479683\n",
            "Epoch 5, Batch 110, Loss: 0.022464144974946976\n",
            "Epoch 5, Batch 120, Loss: 0.022090643644332886\n",
            "Epoch 5, Batch 130, Loss: 0.01833062246441841\n",
            "Epoch 5, Batch 140, Loss: 0.022519350051879883\n",
            "Epoch 5, Batch 150, Loss: 0.01994410902261734\n",
            "Epoch 5, Batch 160, Loss: 0.015871725976467133\n",
            "Epoch 5, Batch 170, Loss: 0.01703876256942749\n",
            "Epoch 5, Batch 180, Loss: 0.02145104669034481\n",
            "Epoch 5, Batch 190, Loss: 0.01845639944076538\n",
            "Epoch 5, Batch 200, Loss: 0.021950524300336838\n",
            "Epoch 5, Batch 210, Loss: 0.02198958769440651\n",
            "Epoch 5, Batch 220, Loss: 0.020252687856554985\n",
            "Epoch 5, Batch 230, Loss: 0.014976656064391136\n",
            "Epoch 5, Batch 240, Loss: 0.018474094569683075\n",
            "Epoch 5, Batch 250, Loss: 0.029980460181832314\n",
            "Epoch 5, Batch 260, Loss: 0.022090893238782883\n",
            "Epoch 5, Batch 270, Loss: 0.020198173820972443\n",
            "Epoch 5, Batch 280, Loss: 0.022036781534552574\n",
            "Epoch 5, Batch 290, Loss: 0.020570183172822\n",
            "Epoch 5, Batch 300, Loss: 0.019360249862074852\n",
            "Epoch 5, Batch 310, Loss: 0.01806005649268627\n",
            "Epoch 5, Batch 320, Loss: 0.02861914038658142\n",
            "Epoch 5, Batch 330, Loss: 0.0199589841067791\n",
            "Epoch 5, Batch 340, Loss: 0.016655100509524345\n",
            "Epoch 5, Batch 350, Loss: 0.021504098549485207\n",
            "Epoch 5, Batch 360, Loss: 0.026509420946240425\n",
            "Epoch 5, Batch 370, Loss: 0.01780419796705246\n",
            "Epoch 5, Batch 380, Loss: 0.020584071055054665\n",
            "Epoch 5, Batch 390, Loss: 0.01397775113582611\n",
            "Epoch 5, Batch 400, Loss: 0.020562095567584038\n",
            "Epoch 5, Batch 410, Loss: 0.021745730191469193\n",
            "Epoch 5, Batch 420, Loss: 0.021831706166267395\n",
            "Epoch 5, Batch 430, Loss: 0.024123460054397583\n",
            "Epoch 5, Batch 440, Loss: 0.021029826253652573\n",
            "Epoch 5, Batch 450, Loss: 0.018706349655985832\n",
            "Epoch 5, Batch 460, Loss: 0.01988702453672886\n",
            "Epoch 5, Batch 470, Loss: 0.017023663967847824\n",
            "Epoch 5, Batch 480, Loss: 0.012908991426229477\n",
            "Epoch 5, Batch 490, Loss: 0.020934786647558212\n",
            "Epoch 5, Batch 500, Loss: 0.018217172473669052\n",
            "Epoch 5, Batch 510, Loss: 0.018790090456604958\n",
            "Epoch 5, Batch 520, Loss: 0.01944732666015625\n",
            "Epoch 5, Batch 530, Loss: 0.02190236747264862\n",
            "Epoch 5, Batch 540, Loss: 0.02276723086833954\n",
            "Epoch 5, Batch 550, Loss: 0.01443307101726532\n",
            "Epoch 5, Batch 560, Loss: 0.024106021970510483\n",
            "Epoch 5, Batch 570, Loss: 0.025365928187966347\n",
            "Epoch 5, Batch 580, Loss: 0.022386187687516212\n",
            "Epoch 5, Batch 590, Loss: 0.018657153472304344\n",
            "Epoch 5, Batch 600, Loss: 0.02437971718609333\n",
            "Epoch 5, Batch 610, Loss: 0.02189275622367859\n",
            "Epoch 5, Batch 620, Loss: 0.022866355255246162\n",
            "Epoch 5, Batch 630, Loss: 0.01685885712504387\n",
            "Epoch 5, Batch 640, Loss: 0.022214578464627266\n",
            "Epoch 5, Batch 650, Loss: 0.017029639333486557\n",
            "Epoch 5, Batch 660, Loss: 0.0194040946662426\n",
            "Epoch 5, Batch 670, Loss: 0.017324374988675117\n",
            "Epoch 5, Batch 680, Loss: 0.012967325747013092\n",
            "Epoch 5, Batch 690, Loss: 0.017045892775058746\n",
            "Epoch 5, Batch 700, Loss: 0.02710726670920849\n",
            "Epoch 5, Batch 710, Loss: 0.02270280010998249\n",
            "Epoch 6, Batch 0, Loss: 0.027756135910749435\n",
            "Epoch 6, Batch 10, Loss: 0.017222562804818153\n",
            "Epoch 6, Batch 20, Loss: 0.017238479107618332\n",
            "Epoch 6, Batch 30, Loss: 0.02182999812066555\n",
            "Epoch 6, Batch 40, Loss: 0.01570594497025013\n",
            "Epoch 6, Batch 50, Loss: 0.02636054717004299\n",
            "Epoch 6, Batch 60, Loss: 0.022121794521808624\n",
            "Epoch 6, Batch 70, Loss: 0.0256646741181612\n",
            "Epoch 6, Batch 80, Loss: 0.02422329969704151\n",
            "Epoch 6, Batch 90, Loss: 0.022711800411343575\n",
            "Epoch 6, Batch 100, Loss: 0.021277843043208122\n",
            "Epoch 6, Batch 110, Loss: 0.023077601566910744\n",
            "Epoch 6, Batch 120, Loss: 0.021902218461036682\n",
            "Epoch 6, Batch 130, Loss: 0.016338564455509186\n",
            "Epoch 6, Batch 140, Loss: 0.019055360928177834\n",
            "Epoch 6, Batch 150, Loss: 0.019544553011655807\n",
            "Epoch 6, Batch 160, Loss: 0.025213195011019707\n",
            "Epoch 6, Batch 170, Loss: 0.01872088387608528\n",
            "Epoch 6, Batch 180, Loss: 0.025906650349497795\n",
            "Epoch 6, Batch 190, Loss: 0.017706818878650665\n",
            "Epoch 6, Batch 200, Loss: 0.023650065064430237\n",
            "Epoch 6, Batch 210, Loss: 0.01781429536640644\n",
            "Epoch 6, Batch 220, Loss: 0.021182842552661896\n",
            "Epoch 6, Batch 230, Loss: 0.030052432790398598\n",
            "Epoch 6, Batch 240, Loss: 0.02125370502471924\n",
            "Epoch 6, Batch 250, Loss: 0.01750997081398964\n",
            "Epoch 6, Batch 260, Loss: 0.02500508725643158\n",
            "Epoch 6, Batch 270, Loss: 0.019361983984708786\n",
            "Epoch 6, Batch 280, Loss: 0.016822680830955505\n",
            "Epoch 6, Batch 290, Loss: 0.016178160905838013\n",
            "Epoch 6, Batch 300, Loss: 0.021973803639411926\n",
            "Epoch 6, Batch 310, Loss: 0.020008329302072525\n",
            "Epoch 6, Batch 320, Loss: 0.0243341363966465\n",
            "Epoch 6, Batch 330, Loss: 0.01637997478246689\n",
            "Epoch 6, Batch 340, Loss: 0.016972234472632408\n",
            "Epoch 6, Batch 350, Loss: 0.016547542065382004\n",
            "Epoch 6, Batch 360, Loss: 0.01639164611697197\n",
            "Epoch 6, Batch 370, Loss: 0.023252218961715698\n",
            "Epoch 6, Batch 380, Loss: 0.025471536442637444\n",
            "Epoch 6, Batch 390, Loss: 0.01611199416220188\n",
            "Epoch 6, Batch 400, Loss: 0.02355438470840454\n",
            "Epoch 6, Batch 410, Loss: 0.02117106132209301\n",
            "Epoch 6, Batch 420, Loss: 0.016612067818641663\n",
            "Epoch 6, Batch 430, Loss: 0.024081721901893616\n",
            "Epoch 6, Batch 440, Loss: 0.02278033085167408\n",
            "Epoch 6, Batch 450, Loss: 0.018693717196583748\n",
            "Epoch 6, Batch 460, Loss: 0.017028355970978737\n",
            "Epoch 6, Batch 470, Loss: 0.022670377045869827\n",
            "Epoch 6, Batch 480, Loss: 0.016081904992461205\n",
            "Epoch 6, Batch 490, Loss: 0.020150896161794662\n",
            "Epoch 6, Batch 500, Loss: 0.01638799160718918\n",
            "Epoch 6, Batch 510, Loss: 0.016102036461234093\n",
            "Epoch 6, Batch 520, Loss: 0.021227596327662468\n",
            "Epoch 6, Batch 530, Loss: 0.022677626460790634\n",
            "Epoch 6, Batch 540, Loss: 0.01759689301252365\n",
            "Epoch 6, Batch 550, Loss: 0.01997525244951248\n",
            "Epoch 6, Batch 560, Loss: 0.019964393228292465\n",
            "Epoch 6, Batch 570, Loss: 0.01906619593501091\n",
            "Epoch 6, Batch 580, Loss: 0.021776221692562103\n",
            "Epoch 6, Batch 590, Loss: 0.020855041220784187\n",
            "Epoch 6, Batch 600, Loss: 0.025560881942510605\n",
            "Epoch 6, Batch 610, Loss: 0.024548562243580818\n",
            "Epoch 6, Batch 620, Loss: 0.01803688518702984\n",
            "Epoch 6, Batch 630, Loss: 0.01574191078543663\n",
            "Epoch 6, Batch 640, Loss: 0.014503734186291695\n",
            "Epoch 6, Batch 650, Loss: 0.01657015085220337\n",
            "Epoch 6, Batch 660, Loss: 0.025188274681568146\n",
            "Epoch 6, Batch 670, Loss: 0.015907589346170425\n",
            "Epoch 6, Batch 680, Loss: 0.014679398387670517\n",
            "Epoch 6, Batch 690, Loss: 0.01793031394481659\n",
            "Epoch 6, Batch 700, Loss: 0.01630144566297531\n",
            "Epoch 6, Batch 710, Loss: 0.014360513538122177\n",
            "Epoch 7, Batch 0, Loss: 0.018217891454696655\n",
            "Epoch 7, Batch 10, Loss: 0.01929943636059761\n",
            "Epoch 7, Batch 20, Loss: 0.015500589273869991\n",
            "Epoch 7, Batch 30, Loss: 0.01584845967590809\n",
            "Epoch 7, Batch 40, Loss: 0.010424247942864895\n",
            "Epoch 7, Batch 50, Loss: 0.03377586230635643\n",
            "Epoch 7, Batch 60, Loss: 0.017897475510835648\n",
            "Epoch 7, Batch 70, Loss: 0.018066659569740295\n",
            "Epoch 7, Batch 80, Loss: 0.029167097061872482\n",
            "Epoch 7, Batch 90, Loss: 0.015002241358160973\n",
            "Epoch 7, Batch 100, Loss: 0.024900393560528755\n",
            "Epoch 7, Batch 110, Loss: 0.02270546369254589\n",
            "Epoch 7, Batch 120, Loss: 0.015007959678769112\n",
            "Epoch 7, Batch 130, Loss: 0.012591767124831676\n",
            "Epoch 7, Batch 140, Loss: 0.020115433260798454\n",
            "Epoch 7, Batch 150, Loss: 0.010788730345666409\n",
            "Epoch 7, Batch 160, Loss: 0.02529231272637844\n",
            "Epoch 7, Batch 170, Loss: 0.024801239371299744\n",
            "Epoch 7, Batch 180, Loss: 0.01668747514486313\n",
            "Epoch 7, Batch 190, Loss: 0.015969645231962204\n",
            "Epoch 7, Batch 200, Loss: 0.015415644273161888\n",
            "Epoch 7, Batch 210, Loss: 0.023033682256937027\n",
            "Epoch 7, Batch 220, Loss: 0.02343462035059929\n",
            "Epoch 7, Batch 230, Loss: 0.026315730065107346\n",
            "Epoch 7, Batch 240, Loss: 0.01889825239777565\n",
            "Epoch 7, Batch 250, Loss: 0.015871891751885414\n",
            "Epoch 7, Batch 260, Loss: 0.014244930818676949\n",
            "Epoch 7, Batch 270, Loss: 0.01823505572974682\n",
            "Epoch 7, Batch 280, Loss: 0.021243546158075333\n",
            "Epoch 7, Batch 290, Loss: 0.01460576243698597\n",
            "Epoch 7, Batch 300, Loss: 0.020076178014278412\n",
            "Epoch 7, Batch 310, Loss: 0.03136717528104782\n",
            "Epoch 7, Batch 320, Loss: 0.021801481023430824\n",
            "Epoch 7, Batch 330, Loss: 0.015400277450680733\n",
            "Epoch 7, Batch 340, Loss: 0.020155083388090134\n",
            "Epoch 7, Batch 350, Loss: 0.01589866727590561\n",
            "Epoch 7, Batch 360, Loss: 0.019904734566807747\n",
            "Epoch 7, Batch 370, Loss: 0.022347107529640198\n",
            "Epoch 7, Batch 380, Loss: 0.017834465950727463\n",
            "Epoch 7, Batch 390, Loss: 0.021551979705691338\n",
            "Epoch 7, Batch 400, Loss: 0.021417414769530296\n",
            "Epoch 7, Batch 410, Loss: 0.02220098488032818\n",
            "Epoch 7, Batch 420, Loss: 0.02379544824361801\n",
            "Epoch 7, Batch 430, Loss: 0.020008409395813942\n",
            "Epoch 7, Batch 440, Loss: 0.01617787778377533\n",
            "Epoch 7, Batch 450, Loss: 0.01300722360610962\n",
            "Epoch 7, Batch 460, Loss: 0.01493406668305397\n",
            "Epoch 7, Batch 470, Loss: 0.01722058653831482\n",
            "Epoch 7, Batch 480, Loss: 0.021843310445547104\n",
            "Epoch 7, Batch 490, Loss: 0.02058175764977932\n",
            "Epoch 7, Batch 500, Loss: 0.015303775668144226\n",
            "Epoch 7, Batch 510, Loss: 0.017439737915992737\n",
            "Epoch 7, Batch 520, Loss: 0.023344814777374268\n",
            "Epoch 7, Batch 530, Loss: 0.013013022020459175\n",
            "Epoch 7, Batch 540, Loss: 0.011365735903382301\n",
            "Epoch 7, Batch 550, Loss: 0.017440427094697952\n",
            "Epoch 7, Batch 560, Loss: 0.021397439762949944\n",
            "Epoch 7, Batch 570, Loss: 0.0220305435359478\n",
            "Epoch 7, Batch 580, Loss: 0.020286474376916885\n",
            "Epoch 7, Batch 590, Loss: 0.021980125457048416\n",
            "Epoch 7, Batch 600, Loss: 0.016109105199575424\n",
            "Epoch 7, Batch 610, Loss: 0.023587975651025772\n",
            "Epoch 7, Batch 620, Loss: 0.01906326413154602\n",
            "Epoch 7, Batch 630, Loss: 0.019986839964985847\n",
            "Epoch 7, Batch 640, Loss: 0.014687999151647091\n",
            "Epoch 7, Batch 650, Loss: 0.01407361589372158\n",
            "Epoch 7, Batch 660, Loss: 0.014606434851884842\n",
            "Epoch 7, Batch 670, Loss: 0.014958538115024567\n",
            "Epoch 7, Batch 680, Loss: 0.019351016730070114\n",
            "Epoch 7, Batch 690, Loss: 0.015662604942917824\n",
            "Epoch 7, Batch 700, Loss: 0.023930901661515236\n",
            "Epoch 7, Batch 710, Loss: 0.019187776371836662\n",
            "Epoch 8, Batch 0, Loss: 0.018210450187325478\n",
            "Epoch 8, Batch 10, Loss: 0.016689317300915718\n",
            "Epoch 8, Batch 20, Loss: 0.019585946574807167\n",
            "Epoch 8, Batch 30, Loss: 0.02108345367014408\n",
            "Epoch 8, Batch 40, Loss: 0.014098460786044598\n",
            "Epoch 8, Batch 50, Loss: 0.023615246638655663\n",
            "Epoch 8, Batch 60, Loss: 0.012213661335408688\n",
            "Epoch 8, Batch 70, Loss: 0.014466394670307636\n",
            "Epoch 8, Batch 80, Loss: 0.018850279971957207\n",
            "Epoch 8, Batch 90, Loss: 0.014951248653233051\n",
            "Epoch 8, Batch 100, Loss: 0.018205292522907257\n",
            "Epoch 8, Batch 110, Loss: 0.014961385168135166\n",
            "Epoch 8, Batch 120, Loss: 0.01849965564906597\n",
            "Epoch 8, Batch 130, Loss: 0.023456647992134094\n",
            "Epoch 8, Batch 140, Loss: 0.021041272208094597\n",
            "Epoch 8, Batch 150, Loss: 0.018109826371073723\n",
            "Epoch 8, Batch 160, Loss: 0.020823540166020393\n",
            "Epoch 8, Batch 170, Loss: 0.014556639827787876\n",
            "Epoch 8, Batch 180, Loss: 0.02148192562162876\n",
            "Epoch 8, Batch 190, Loss: 0.018493901938199997\n",
            "Epoch 8, Batch 200, Loss: 0.018980365246534348\n",
            "Epoch 8, Batch 210, Loss: 0.013200161047279835\n",
            "Epoch 8, Batch 220, Loss: 0.016057638451457024\n",
            "Epoch 8, Batch 230, Loss: 0.01777144894003868\n",
            "Epoch 8, Batch 240, Loss: 0.017577216029167175\n",
            "Epoch 8, Batch 250, Loss: 0.022859282791614532\n",
            "Epoch 8, Batch 260, Loss: 0.020205117762088776\n",
            "Epoch 8, Batch 270, Loss: 0.023755846545100212\n",
            "Epoch 8, Batch 280, Loss: 0.03348250314593315\n",
            "Epoch 8, Batch 290, Loss: 0.014745373278856277\n",
            "Epoch 8, Batch 300, Loss: 0.021164651960134506\n",
            "Epoch 8, Batch 310, Loss: 0.012006941251456738\n",
            "Epoch 8, Batch 320, Loss: 0.02286611869931221\n",
            "Epoch 8, Batch 330, Loss: 0.015760459005832672\n",
            "Epoch 8, Batch 340, Loss: 0.012699153274297714\n",
            "Epoch 8, Batch 350, Loss: 0.018145909532904625\n",
            "Epoch 8, Batch 360, Loss: 0.01537218689918518\n",
            "Epoch 8, Batch 370, Loss: 0.015316085889935493\n",
            "Epoch 8, Batch 380, Loss: 0.015298988670110703\n",
            "Epoch 8, Batch 390, Loss: 0.01725906878709793\n",
            "Epoch 8, Batch 400, Loss: 0.018883373588323593\n",
            "Epoch 8, Batch 410, Loss: 0.018418531864881516\n",
            "Epoch 8, Batch 420, Loss: 0.017859168350696564\n",
            "Epoch 8, Batch 430, Loss: 0.021388504654169083\n",
            "Epoch 8, Batch 440, Loss: 0.019568413496017456\n",
            "Epoch 8, Batch 450, Loss: 0.012206587940454483\n",
            "Epoch 8, Batch 460, Loss: 0.015050049871206284\n",
            "Epoch 8, Batch 470, Loss: 0.01757390797138214\n",
            "Epoch 8, Batch 480, Loss: 0.014887156896293163\n",
            "Epoch 8, Batch 490, Loss: 0.020259886980056763\n",
            "Epoch 8, Batch 500, Loss: 0.015175900422036648\n",
            "Epoch 8, Batch 510, Loss: 0.017861589789390564\n",
            "Epoch 8, Batch 520, Loss: 0.018137304112315178\n",
            "Epoch 8, Batch 530, Loss: 0.01821247860789299\n",
            "Epoch 8, Batch 540, Loss: 0.013816704042255878\n",
            "Epoch 8, Batch 550, Loss: 0.02167322486639023\n",
            "Epoch 8, Batch 560, Loss: 0.01887591741979122\n",
            "Epoch 8, Batch 570, Loss: 0.016760699450969696\n",
            "Epoch 8, Batch 580, Loss: 0.01279699057340622\n",
            "Epoch 8, Batch 590, Loss: 0.020044703036546707\n",
            "Epoch 8, Batch 600, Loss: 0.012369425967335701\n",
            "Epoch 8, Batch 610, Loss: 0.022093044593930244\n",
            "Epoch 8, Batch 620, Loss: 0.01773166097700596\n",
            "Epoch 8, Batch 630, Loss: 0.01716233603656292\n",
            "Epoch 8, Batch 640, Loss: 0.02564121037721634\n",
            "Epoch 8, Batch 650, Loss: 0.014776645228266716\n",
            "Epoch 8, Batch 660, Loss: 0.013753754086792469\n",
            "Epoch 8, Batch 670, Loss: 0.017819475382566452\n",
            "Epoch 8, Batch 680, Loss: 0.015056900680065155\n",
            "Epoch 8, Batch 690, Loss: 0.02195996418595314\n",
            "Epoch 8, Batch 700, Loss: 0.016477247700095177\n",
            "Epoch 8, Batch 710, Loss: 0.020280344411730766\n",
            "Epoch 9, Batch 0, Loss: 0.010924654081463814\n",
            "Epoch 9, Batch 10, Loss: 0.011944754980504513\n",
            "Epoch 9, Batch 20, Loss: 0.014887973666191101\n",
            "Epoch 9, Batch 30, Loss: 0.01255761831998825\n",
            "Epoch 9, Batch 40, Loss: 0.023540165275335312\n",
            "Epoch 9, Batch 50, Loss: 0.017456281930208206\n",
            "Epoch 9, Batch 60, Loss: 0.01972106099128723\n",
            "Epoch 9, Batch 70, Loss: 0.011945118196308613\n",
            "Epoch 9, Batch 80, Loss: 0.01823464222252369\n",
            "Epoch 9, Batch 90, Loss: 0.01236073300242424\n",
            "Epoch 9, Batch 100, Loss: 0.013761000707745552\n",
            "Epoch 9, Batch 110, Loss: 0.013430476188659668\n",
            "Epoch 9, Batch 120, Loss: 0.008901446126401424\n",
            "Epoch 9, Batch 130, Loss: 0.010736650787293911\n",
            "Epoch 9, Batch 140, Loss: 0.02003786340355873\n",
            "Epoch 9, Batch 150, Loss: 0.015669137239456177\n",
            "Epoch 9, Batch 160, Loss: 0.016413191333413124\n",
            "Epoch 9, Batch 170, Loss: 0.015099800191819668\n",
            "Epoch 9, Batch 180, Loss: 0.017130248248577118\n",
            "Epoch 9, Batch 190, Loss: 0.024661783128976822\n",
            "Epoch 9, Batch 200, Loss: 0.013952373526990414\n",
            "Epoch 9, Batch 210, Loss: 0.015669960528612137\n",
            "Epoch 9, Batch 220, Loss: 0.01155393198132515\n",
            "Epoch 9, Batch 230, Loss: 0.022073743864893913\n",
            "Epoch 9, Batch 240, Loss: 0.011296242475509644\n",
            "Epoch 9, Batch 250, Loss: 0.016359057277441025\n",
            "Epoch 9, Batch 260, Loss: 0.029945043846964836\n",
            "Epoch 9, Batch 270, Loss: 0.02572857029736042\n",
            "Epoch 9, Batch 280, Loss: 0.017035719007253647\n",
            "Epoch 9, Batch 290, Loss: 0.018412332981824875\n",
            "Epoch 9, Batch 300, Loss: 0.01576421968638897\n",
            "Epoch 9, Batch 310, Loss: 0.01542256586253643\n",
            "Epoch 9, Batch 320, Loss: 0.01600039377808571\n",
            "Epoch 9, Batch 330, Loss: 0.015362393110990524\n",
            "Epoch 9, Batch 340, Loss: 0.014930951409041882\n",
            "Epoch 9, Batch 350, Loss: 0.01848672516644001\n",
            "Epoch 9, Batch 360, Loss: 0.0211187731474638\n",
            "Epoch 9, Batch 370, Loss: 0.014640461653470993\n",
            "Epoch 9, Batch 380, Loss: 0.014927474781870842\n",
            "Epoch 9, Batch 390, Loss: 0.015812724828720093\n",
            "Epoch 9, Batch 400, Loss: 0.01560058444738388\n",
            "Epoch 9, Batch 410, Loss: 0.015284129418432713\n",
            "Epoch 9, Batch 420, Loss: 0.01393851824104786\n",
            "Epoch 9, Batch 430, Loss: 0.015573311597108841\n",
            "Epoch 9, Batch 440, Loss: 0.018330249935388565\n",
            "Epoch 9, Batch 450, Loss: 0.021207284182310104\n",
            "Epoch 9, Batch 460, Loss: 0.012660661712288857\n",
            "Epoch 9, Batch 470, Loss: 0.024941351264715195\n",
            "Epoch 9, Batch 480, Loss: 0.020345572382211685\n",
            "Epoch 9, Batch 490, Loss: 0.010503250174224377\n",
            "Epoch 9, Batch 500, Loss: 0.021103713661432266\n",
            "Epoch 9, Batch 510, Loss: 0.016992688179016113\n",
            "Epoch 9, Batch 520, Loss: 0.016926541924476624\n",
            "Epoch 9, Batch 530, Loss: 0.017771419137716293\n",
            "Epoch 9, Batch 540, Loss: 0.010266896337270737\n",
            "Epoch 9, Batch 550, Loss: 0.019949598237872124\n",
            "Epoch 9, Batch 560, Loss: 0.015102270990610123\n",
            "Epoch 9, Batch 570, Loss: 0.019661109894514084\n",
            "Epoch 9, Batch 580, Loss: 0.015712087973952293\n",
            "Epoch 9, Batch 590, Loss: 0.02126145549118519\n",
            "Epoch 9, Batch 600, Loss: 0.011466467753052711\n",
            "Epoch 9, Batch 610, Loss: 0.01656755432486534\n",
            "Epoch 9, Batch 620, Loss: 0.014820327050983906\n",
            "Epoch 9, Batch 630, Loss: 0.0284329392015934\n",
            "Epoch 9, Batch 640, Loss: 0.017024297267198563\n",
            "Epoch 9, Batch 650, Loss: 0.015630392357707024\n",
            "Epoch 9, Batch 660, Loss: 0.023767001926898956\n",
            "Epoch 9, Batch 670, Loss: 0.018350619822740555\n",
            "Epoch 9, Batch 680, Loss: 0.017804639413952827\n",
            "Epoch 9, Batch 690, Loss: 0.015229075215756893\n",
            "Epoch 9, Batch 700, Loss: 0.014253641478717327\n",
            "Epoch 9, Batch 710, Loss: 0.019368648529052734\n"
          ]
        }
      ],
      "source": [
        "# Set the print interval\n",
        "print_interval = 10  # Adjust the interval as needed\n",
        "\n",
        "# Cell 12: Training loop with performance monitoring\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_index, batch in enumerate(train_dataloader):\n",
        "        user, course, grade = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print input shapes for debugging\n",
        "        #print(\"Shapes - User:\", user.shape, \"Course:\", course.shape)\n",
        "\n",
        "        output = model(user, course).squeeze()\n",
        "        loss = criterion(output, grade)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training loss at regular intervals\n",
        "        if batch_index % print_interval == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_index}, Loss: {loss.item()}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-MRNKF74X7R",
        "outputId": "4bb01049-c05d-444b-9545-270f8c63dedb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) on the test dataset: 0.00025130533620206804\n"
          ]
        }
      ],
      "source": [
        "# Cell 14: Evaluation on the test dataset\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_dataset = TensorDataset(test_user, test_course, test_grade)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "total_test_loss = 0.0\n",
        "num_test_samples = 0\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        user, course, grade = batch\n",
        "        output = model(user, course).squeeze()\n",
        "        loss = criterion(output, grade)\n",
        "        total_test_loss += loss.item()\n",
        "        num_test_samples += len(grade)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) on the test dataset\n",
        "test_mse = total_test_loss / num_test_samples\n",
        "\n",
        "print(f'Mean Squared Error (MSE) on the test dataset: {test_mse}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpQW166kB5fv"
      },
      "outputs": [],
      "source": [
        "#To evaluate model based on hit ratio\n",
        "def hit_ratio_at_k(model, test_data, le_student, le_course, top_k):\n",
        "    model.eval()\n",
        "    num_hits = 0\n",
        "    total_users = len(test_data['student_id'].unique())\n",
        "\n",
        "    for user_id in test_data['student_id'].unique():\n",
        "        user = torch.LongTensor([le_student.transform([user_id])[0]])\n",
        "        all_course_ids = torch.arange(len(le_course.classes_))\n",
        "        user_ids = torch.full_like(all_course_ids, fill_value=user.item())\n",
        "\n",
        "        predictions = model(user_ids, all_course_ids).squeeze()\n",
        "\n",
        "        top_indices = torch.topk(predictions, top_k).indices.numpy()\n",
        "        top_course_ids = le_course.inverse_transform(top_indices)\n",
        "\n",
        "        user_data = test_data[test_data['student_id'] == user_id]\n",
        "        true_courses = user_data['course_id'].values\n",
        "\n",
        "        # Check if any of the true courses is among the top-k recommended courses\n",
        "        if any(course in top_course_ids for course in true_courses):\n",
        "            num_hits += 1\n",
        "\n",
        "    hit_ratio = num_hits / total_users\n",
        "    return hit_ratio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAiL-mWuB7XO",
        "outputId": "5a08b59b-cac4-4ac7-8b25-14713e015959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit Ratio at 5: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Set top_k to the desired number for hit ratio calculation\n",
        "top_k = 5\n",
        "# Evaluate Hit Ratio at k on the test set\n",
        "hit_ratio = hit_ratio_at_k(model, test_df, le_student, le_course, top_k)\n",
        "print(f\"Hit Ratio at {top_k}: {hit_ratio}\")\n",
        "#we are getting 0->it means students chosen courses isnt the same as the recommeded courses\n",
        "#this isnt an effective measure of model performance because ts not necessary that students wud have\n",
        "#chosen the recommended courses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qscJn_4qmIp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd6a613-6575-4277-dff5-4d1f31182c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommended courses for the student based on previous grades: ['PRACTICE SCHOOL II', 'STUDY PROJECT', 'THESIS', 'CONTROL SYSTEMS LABORATORY', 'ELEC & ELECTRONIC CIRCUITS LAB']\n"
          ]
        }
      ],
      "source": [
        "def recommend_courses(model, user_and_courses, top_k):\n",
        "    input_user, input_courses = user_and_courses\n",
        "\n",
        "    # Convert input_user to PyTorch tensor\n",
        "    user_ids = torch.LongTensor([input_user])\n",
        "\n",
        "    # Generate all possible course IDs\n",
        "    all_course_ids = torch.arange(len(le_course.classes_))\n",
        "\n",
        "    # Repeat the given user_id for all courses\n",
        "    user_ids = torch.full_like(all_course_ids, fill_value=user_ids[0])\n",
        "\n",
        "    # Make predictions for all courses for the given student\n",
        "    predictions = model(user_ids, all_course_ids).squeeze()\n",
        "\n",
        "    # Exclude courses already in input_courses from recommendations\n",
        "    for course, grade in input_courses.items():\n",
        "        course_index = le_course.transform([course])[0]\n",
        "        if course_index < len(predictions):\n",
        "            predictions[course_index] = float('-inf')\n",
        "\n",
        "    # Get the indices of the top-k predictions\n",
        "    num_recommendations = min(top_k, len(predictions))\n",
        "    top_indices = torch.topk(predictions, num_recommendations).indices\n",
        "\n",
        "    # Map the top indices back to the course IDs\n",
        "    top_course_ids = le_course.inverse_transform(top_indices.numpy())\n",
        "\n",
        "    # Exclude courses already in input_courses from recommendations (additional check)\n",
        "    top_course_ids = [course_id for course_id in top_course_ids if course_id not in input_courses]\n",
        "\n",
        "    return top_course_ids\n",
        "\n",
        "# Example: Input student's grade for previous courses\n",
        "input_user = 123  # Replace with the actual student ID\n",
        "\n",
        "# Use the actual course labels seen during training\n",
        "input_courses = {\n",
        "    'CHEMISTRY LABORATORY': 8,\n",
        "    'GENERAL CHEMISTRY': 7,\n",
        "    'ELECTRICAL SCIENCES': 3,\n",
        "    'ADDITIVE MANUFACTURING': 1,\n",
        "    'PRACTICE SCHOOL I':10,\n",
        "    'PHYSICS LABORATORY':5\n",
        "\n",
        "    # Add more courses and grades as needed\n",
        "}\n",
        "\n",
        "# Set top_k to the desired number\n",
        "top_k = 5\n",
        "\n",
        "# Call the function with the updated course grades\n",
        "user_and_courses = (input_user, input_courses)\n",
        "recommended_courses = recommend_courses(model, user_and_courses, top_k)\n",
        "\n",
        "print(f\"Top {top_k} recommended courses for the student based on previous grades: {recommended_courses}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "j2FAU6TsWva-",
        "outputId": "3b6e03f8-019c-471e-d7c2-8e9c24e314b3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Tensors must have same number of dimensions: got 3 and 2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a591404c9e6b>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Call the function with the updated course grades\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mrecommended_courses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommend_courses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_grades\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Top {top_k} recommended courses for the student based on previous grades: {recommended_courses}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a591404c9e6b>\u001b[0m in \u001b[0;36mrecommend_courses\u001b[0;34m(model, user_grades, top_k)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Make predictions for all courses for the given user grades\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_grades_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_course_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to LongTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Get the indices of the top-k predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fde1084135a5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, user, course)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Concatenate user and course embeddings along the last dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0minteraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcourse_emb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
          ]
        }
      ],
      "source": [
        "def recommend_courses(model, user_grades, top_k):\n",
        "    # Convert user grades to PyTorch tensor\n",
        "    user_grades_tensor = torch.FloatTensor(user_grades)\n",
        "\n",
        "    # Generate all possible course IDs\n",
        "    all_course_ids = torch.arange(len(le_course.classes_))\n",
        "\n",
        "    # Repeat the given user grades for all courses\n",
        "    user_grades_tensor = user_grades_tensor.unsqueeze(0).expand(len(le_course.classes_), -1)\n",
        "\n",
        "    # Make predictions for all courses for the given user grades\n",
        "    predictions = model(user_grades_tensor.long(), all_course_ids).squeeze()  # Convert to LongTensor\n",
        "\n",
        "    # Get the indices of the top-k predictions\n",
        "    num_recommendations = min(top_k, len(predictions))\n",
        "    top_indices = torch.topk(predictions, num_recommendations).indices\n",
        "\n",
        "    # Map the top indices back to the course IDs\n",
        "    top_course_ids = le_course.inverse_transform(top_indices.numpy())\n",
        "\n",
        "    return top_course_ids\n",
        "\n",
        "\n",
        "# Example: Input student's grade for previous courses\n",
        "input_grades = {\n",
        "    'CHEMISTRY LABORATORY': 8,\n",
        "    'GENERAL CHEMISTRY': 7,\n",
        "    'ELECTRICAL SCIENCES': 3,\n",
        "    'ADDITIVE MANUFACTURING': 1,\n",
        "    'PRACTICE SCHOOL I': 10,\n",
        "    'PHYSICS LABORATORY': 5\n",
        "    # Add more courses and grades as needed\n",
        "}\n",
        "\n",
        "# Set top_k to the desired number\n",
        "top_k = 5\n",
        "\n",
        "# Call the function with the updated course grades\n",
        "recommended_courses = recommend_courses(model, list(input_grades.values()), top_k)\n",
        "\n",
        "print(f\"Top {top_k} recommended courses for the student based on previous grades: {recommended_courses}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPuwUtRcU1Tg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND4u/JpAuvVzRT/X/BWYG6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}